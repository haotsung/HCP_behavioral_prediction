{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "from junifer.storage import HDF5FeatureStorage\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from julearn import run_cross_validation\n",
    "from julearn.pipeline import PipelineCreator\n",
    "from julearn.utils import configure_logging\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from julearn.config import set_config\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80 specific subjects same as the paper\n",
    "train_subs = pd.read_csv(\n",
    "    \"MMP_HCP_80_subs_componentscoreestimation.txt\", header=None\n",
    ").values.flatten().astype(str)  # 80 subjects\n",
    "\n",
    "# 753 specific subject for main analysis same as the paper\n",
    "test_subs = pd.read_csv(\n",
    "    \"MMP_HCP_753_subs.txt\", header=None\n",
    ").values.flatten().astype(str)  # 753 subjects\n",
    "\n",
    "columns= ['PicSeq_Unadj','CardSort_Unadj','Flanker_Unadj','PMAT24_A_CR','ReadEng_Unadj','PicVocab_Unadj','ProcSpeed_Unadj','VSPLOT_TC','SCPT_SEN','SCPT_SPEC','IWRD_TOT','ListSort_Unadj','MMSE_Score',\n",
    "                     'PSQI_Score','Endurance_Unadj','Dexterity_Unadj','Strength_Unadj','Odor_Unadj','PainInterf_Tscore','Taste_Unadj','Mars_Final','Emotion_Task_Face_Acc','Language_Task_Math_Avg_Difficulty_Level',\n",
    "                     'Language_Task_Story_Avg_Difficulty_Level','Relational_Task_Acc','Social_Task_Perc_Random','Social_Task_Perc_TOM','WM_Task_Acc','NEOFAC_A','NEOFAC_O','NEOFAC_C','NEOFAC_N','NEOFAC_E','ER40_CR','ER40ANG','ER40FEAR',\n",
    "                     'ER40HAP','ER40NOE','ER40SAD','AngAffect_Unadj','AngHostil_Unadj','AngAggr_Unadj','FearAffect_Unadj','FearSomat_Unadj','Sadness_Unadj','LifeSatisf_Unadj','MeanPurp_Unadj','PosAffect_Unadj','Friendship_Unadj',\n",
    "                     'Loneliness_Unadj','PercHostil_Unadj','PercReject_Unadj','EmotSupp_Unadj','InstruSupp_Unadj','PercStress_Unadj','SelfEff_Unadj','DDisc_AUC_40K','GaitSpeed_Comp']\n",
    "# Load the dataset\n",
    "full_df = pd.read_csv(\"Behavioral_Data\", index_col=\"Subject\")[columns]\n",
    "full_df.index = full_df.index.astype(str)\n",
    "test_df = full_df.loc[test_subs]\n",
    "train_df = full_df.loc[train_subs]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "imputer = IterativeImputer(max_iter=20, random_state=0)\n",
    "train_df_imputed = imputer.fit_transform(train_df)\n",
    "train_df_scaled = scaler.fit_transform(train_df_imputed)\n",
    "test_df_imputed = imputer.transform(test_df)\n",
    "test_df_scaled = scaler.transform(test_df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 1.0121, Validation Loss: 0.9083\n",
      "Epoch 2/500, Loss: 1.0085, Validation Loss: 0.9039\n",
      "Epoch 3/500, Loss: 1.0053, Validation Loss: 0.9004\n",
      "Epoch 4/500, Loss: 1.0022, Validation Loss: 0.8978\n",
      "Epoch 5/500, Loss: 1.0015, Validation Loss: 0.8951\n",
      "Epoch 6/500, Loss: 0.9983, Validation Loss: 0.8919\n",
      "Epoch 7/500, Loss: 0.9958, Validation Loss: 0.8875\n",
      "Epoch 8/500, Loss: 0.9918, Validation Loss: 0.8804\n",
      "Epoch 9/500, Loss: 0.9873, Validation Loss: 0.8692\n",
      "Epoch 10/500, Loss: 0.9784, Validation Loss: 0.8526\n",
      "Epoch 11/500, Loss: 0.9656, Validation Loss: 0.8311\n",
      "Epoch 12/500, Loss: 0.9463, Validation Loss: 0.8125\n",
      "Epoch 13/500, Loss: 0.9363, Validation Loss: 0.8086\n",
      "Epoch 14/500, Loss: 0.9247, Validation Loss: 0.7978\n",
      "Epoch 15/500, Loss: 0.9189, Validation Loss: 0.7901\n",
      "Epoch 16/500, Loss: 0.8974, Validation Loss: 0.7926\n",
      "Epoch 17/500, Loss: 0.8833, Validation Loss: 0.7970\n",
      "Epoch 18/500, Loss: 0.8811, Validation Loss: 0.8003\n",
      "Epoch 19/500, Loss: 0.8678, Validation Loss: 0.8048\n",
      "Epoch 20/500, Loss: 0.8666, Validation Loss: 0.8014\n",
      "Epoch 21/500, Loss: 0.8620, Validation Loss: 0.7943\n",
      "Epoch 22/500, Loss: 0.8504, Validation Loss: 0.7890\n",
      "Epoch 23/500, Loss: 0.8447, Validation Loss: 0.7874\n",
      "Epoch 24/500, Loss: 0.8515, Validation Loss: 0.7871\n",
      "Epoch 25/500, Loss: 0.8392, Validation Loss: 0.7857\n",
      "Epoch 26/500, Loss: 0.8364, Validation Loss: 0.7838\n",
      "Epoch 27/500, Loss: 0.8251, Validation Loss: 0.7839\n",
      "Epoch 28/500, Loss: 0.8259, Validation Loss: 0.7856\n",
      "Epoch 29/500, Loss: 0.8278, Validation Loss: 0.7871\n",
      "Epoch 30/500, Loss: 0.8236, Validation Loss: 0.7868\n",
      "Epoch 31/500, Loss: 0.8210, Validation Loss: 0.7873\n",
      "Epoch 32/500, Loss: 0.8186, Validation Loss: 0.7864\n",
      "Epoch 33/500, Loss: 0.8175, Validation Loss: 0.7867\n",
      "Epoch 34/500, Loss: 0.8086, Validation Loss: 0.7858\n",
      "Epoch 35/500, Loss: 0.8087, Validation Loss: 0.7852\n",
      "Epoch 36/500, Loss: 0.8201, Validation Loss: 0.7843\n",
      "Epoch 37/500, Loss: 0.8201, Validation Loss: 0.7861\n",
      "Epoch 38/500, Loss: 0.8076, Validation Loss: 0.7826\n",
      "Epoch 39/500, Loss: 0.8052, Validation Loss: 0.7791\n",
      "Epoch 40/500, Loss: 0.7978, Validation Loss: 0.7800\n",
      "Epoch 41/500, Loss: 0.8000, Validation Loss: 0.7805\n",
      "Epoch 42/500, Loss: 0.8027, Validation Loss: 0.7795\n",
      "Epoch 43/500, Loss: 0.8092, Validation Loss: 0.7775\n",
      "Epoch 44/500, Loss: 0.7996, Validation Loss: 0.7813\n",
      "Epoch 45/500, Loss: 0.7991, Validation Loss: 0.7845\n",
      "Epoch 46/500, Loss: 0.8152, Validation Loss: 0.7798\n",
      "Epoch 47/500, Loss: 0.7895, Validation Loss: 0.7779\n",
      "Epoch 48/500, Loss: 0.7824, Validation Loss: 0.7789\n",
      "Epoch 49/500, Loss: 0.7906, Validation Loss: 0.7772\n",
      "Epoch 50/500, Loss: 0.7828, Validation Loss: 0.7763\n",
      "Epoch 51/500, Loss: 0.7960, Validation Loss: 0.7752\n",
      "Epoch 52/500, Loss: 0.7958, Validation Loss: 0.7730\n",
      "Epoch 53/500, Loss: 0.7822, Validation Loss: 0.7726\n",
      "Epoch 54/500, Loss: 0.7811, Validation Loss: 0.7744\n",
      "Epoch 55/500, Loss: 0.7812, Validation Loss: 0.7752\n",
      "Epoch 56/500, Loss: 0.7810, Validation Loss: 0.7759\n",
      "Epoch 57/500, Loss: 0.7978, Validation Loss: 0.7721\n",
      "Epoch 58/500, Loss: 0.7810, Validation Loss: 0.7704\n",
      "Epoch 59/500, Loss: 0.7710, Validation Loss: 0.7700\n",
      "Epoch 60/500, Loss: 0.7710, Validation Loss: 0.7699\n",
      "Epoch 61/500, Loss: 0.7685, Validation Loss: 0.7686\n",
      "Epoch 62/500, Loss: 0.7645, Validation Loss: 0.7673\n",
      "Epoch 63/500, Loss: 0.7688, Validation Loss: 0.7667\n",
      "Epoch 64/500, Loss: 0.7596, Validation Loss: 0.7682\n",
      "Epoch 65/500, Loss: 0.7625, Validation Loss: 0.7694\n",
      "Epoch 66/500, Loss: 0.7614, Validation Loss: 0.7685\n",
      "Epoch 67/500, Loss: 0.7604, Validation Loss: 0.7686\n",
      "Epoch 68/500, Loss: 0.7568, Validation Loss: 0.7685\n",
      "Epoch 69/500, Loss: 0.7566, Validation Loss: 0.7701\n",
      "Epoch 70/500, Loss: 0.7515, Validation Loss: 0.7692\n",
      "Epoch 71/500, Loss: 0.7576, Validation Loss: 0.7665\n",
      "Epoch 72/500, Loss: 0.7588, Validation Loss: 0.7646\n",
      "Epoch 73/500, Loss: 0.7501, Validation Loss: 0.7648\n",
      "Epoch 74/500, Loss: 0.7501, Validation Loss: 0.7669\n",
      "Epoch 75/500, Loss: 0.7481, Validation Loss: 0.7700\n",
      "Epoch 76/500, Loss: 0.7593, Validation Loss: 0.7718\n",
      "Epoch 77/500, Loss: 0.7470, Validation Loss: 0.7702\n",
      "Epoch 78/500, Loss: 0.7448, Validation Loss: 0.7698\n",
      "Epoch 79/500, Loss: 0.7565, Validation Loss: 0.7701\n",
      "Epoch 80/500, Loss: 0.7447, Validation Loss: 0.7708\n",
      "Epoch 81/500, Loss: 0.7499, Validation Loss: 0.7703\n",
      "Epoch 82/500, Loss: 0.7354, Validation Loss: 0.7699\n",
      "Epoch 83/500, Loss: 0.7345, Validation Loss: 0.7679\n",
      "Epoch 84/500, Loss: 0.7402, Validation Loss: 0.7659\n",
      "Epoch 85/500, Loss: 0.7385, Validation Loss: 0.7652\n",
      "Epoch 86/500, Loss: 0.7488, Validation Loss: 0.7645\n",
      "Epoch 87/500, Loss: 0.7291, Validation Loss: 0.7627\n",
      "Epoch 88/500, Loss: 0.7239, Validation Loss: 0.7616\n",
      "Epoch 89/500, Loss: 0.7507, Validation Loss: 0.7612\n",
      "Epoch 90/500, Loss: 0.7376, Validation Loss: 0.7606\n",
      "Epoch 91/500, Loss: 0.7325, Validation Loss: 0.7615\n",
      "Epoch 92/500, Loss: 0.7240, Validation Loss: 0.7625\n",
      "Epoch 93/500, Loss: 0.7351, Validation Loss: 0.7602\n",
      "Epoch 94/500, Loss: 0.7138, Validation Loss: 0.7574\n",
      "Epoch 95/500, Loss: 0.7197, Validation Loss: 0.7561\n",
      "Epoch 96/500, Loss: 0.7255, Validation Loss: 0.7558\n",
      "Epoch 97/500, Loss: 0.7331, Validation Loss: 0.7553\n",
      "Epoch 98/500, Loss: 0.7319, Validation Loss: 0.7515\n",
      "Epoch 99/500, Loss: 0.7248, Validation Loss: 0.7498\n",
      "Epoch 100/500, Loss: 0.7092, Validation Loss: 0.7496\n",
      "Epoch 101/500, Loss: 0.7259, Validation Loss: 0.7503\n",
      "Epoch 102/500, Loss: 0.7139, Validation Loss: 0.7500\n",
      "Epoch 103/500, Loss: 0.7205, Validation Loss: 0.7456\n",
      "Epoch 104/500, Loss: 0.7155, Validation Loss: 0.7442\n",
      "Epoch 105/500, Loss: 0.7129, Validation Loss: 0.7457\n",
      "Epoch 106/500, Loss: 0.7149, Validation Loss: 0.7455\n",
      "Epoch 107/500, Loss: 0.7184, Validation Loss: 0.7446\n",
      "Epoch 108/500, Loss: 0.7023, Validation Loss: 0.7479\n",
      "Epoch 109/500, Loss: 0.7120, Validation Loss: 0.7496\n",
      "Epoch 110/500, Loss: 0.7180, Validation Loss: 0.7484\n",
      "Epoch 111/500, Loss: 0.7206, Validation Loss: 0.7466\n",
      "Epoch 112/500, Loss: 0.7065, Validation Loss: 0.7456\n",
      "Epoch 113/500, Loss: 0.7028, Validation Loss: 0.7448\n",
      "Epoch 114/500, Loss: 0.7007, Validation Loss: 0.7443\n",
      "Epoch 115/500, Loss: 0.7166, Validation Loss: 0.7459\n",
      "Epoch 116/500, Loss: 0.7138, Validation Loss: 0.7432\n",
      "Epoch 117/500, Loss: 0.6974, Validation Loss: 0.7408\n",
      "Epoch 118/500, Loss: 0.7083, Validation Loss: 0.7425\n",
      "Epoch 119/500, Loss: 0.7060, Validation Loss: 0.7407\n",
      "Epoch 120/500, Loss: 0.7102, Validation Loss: 0.7413\n",
      "Epoch 121/500, Loss: 0.7013, Validation Loss: 0.7441\n",
      "Epoch 122/500, Loss: 0.7142, Validation Loss: 0.7442\n",
      "Epoch 123/500, Loss: 0.7031, Validation Loss: 0.7465\n",
      "Epoch 124/500, Loss: 0.6974, Validation Loss: 0.7476\n",
      "Epoch 125/500, Loss: 0.6994, Validation Loss: 0.7473\n",
      "Epoch 126/500, Loss: 0.6992, Validation Loss: 0.7463\n",
      "Epoch 127/500, Loss: 0.7031, Validation Loss: 0.7472\n",
      "Epoch 128/500, Loss: 0.6970, Validation Loss: 0.7476\n",
      "Epoch 129/500, Loss: 0.6956, Validation Loss: 0.7463\n",
      "Epoch 130/500, Loss: 0.7004, Validation Loss: 0.7456\n",
      "Epoch 131/500, Loss: 0.6982, Validation Loss: 0.7482\n",
      "Epoch 132/500, Loss: 0.6955, Validation Loss: 0.7486\n",
      "Epoch 133/500, Loss: 0.7014, Validation Loss: 0.7469\n",
      "Epoch 134/500, Loss: 0.7052, Validation Loss: 0.7441\n",
      "Epoch 135/500, Loss: 0.7019, Validation Loss: 0.7436\n",
      "Epoch 136/500, Loss: 0.7404, Validation Loss: 0.7471\n",
      "Epoch 137/500, Loss: 0.7001, Validation Loss: 0.7508\n",
      "Epoch 138/500, Loss: 0.7007, Validation Loss: 0.7489\n",
      "Epoch 139/500, Loss: 0.7006, Validation Loss: 0.7461\n",
      "Epoch 140/500, Loss: 0.7071, Validation Loss: 0.7468\n",
      "Epoch 141/500, Loss: 0.6897, Validation Loss: 0.7479\n",
      "Epoch 142/500, Loss: 0.7164, Validation Loss: 0.7492\n",
      "Epoch 143/500, Loss: 0.7002, Validation Loss: 0.7477\n",
      "Epoch 144/500, Loss: 0.6922, Validation Loss: 0.7423\n",
      "Epoch 145/500, Loss: 0.6791, Validation Loss: 0.7525\n",
      "Epoch 146/500, Loss: 0.6993, Validation Loss: 0.7645\n",
      "Epoch 147/500, Loss: 0.7117, Validation Loss: 0.7609\n",
      "Epoch 148/500, Loss: 0.7078, Validation Loss: 0.7486\n",
      "Epoch 149/500, Loss: 0.6895, Validation Loss: 0.7408\n",
      "Epoch 150/500, Loss: 0.6817, Validation Loss: 0.7437\n",
      "Epoch 151/500, Loss: 0.6870, Validation Loss: 0.7480\n",
      "Epoch 152/500, Loss: 0.6970, Validation Loss: 0.7483\n",
      "Epoch 153/500, Loss: 0.6926, Validation Loss: 0.7449\n",
      "Epoch 154/500, Loss: 0.6787, Validation Loss: 0.7454\n",
      "Epoch 155/500, Loss: 0.6879, Validation Loss: 0.7475\n",
      "Epoch 156/500, Loss: 0.6830, Validation Loss: 0.7470\n",
      "Epoch 157/500, Loss: 0.6833, Validation Loss: 0.7449\n",
      "Epoch 158/500, Loss: 0.6812, Validation Loss: 0.7438\n",
      "Epoch 159/500, Loss: 0.6830, Validation Loss: 0.7439\n",
      "Epoch 160/500, Loss: 0.6951, Validation Loss: 0.7436\n",
      "Epoch 161/500, Loss: 0.6866, Validation Loss: 0.7440\n",
      "Epoch 162/500, Loss: 0.6794, Validation Loss: 0.7444\n",
      "Epoch 163/500, Loss: 0.6803, Validation Loss: 0.7439\n",
      "Epoch 164/500, Loss: 0.6775, Validation Loss: 0.7436\n",
      "Epoch 165/500, Loss: 0.6916, Validation Loss: 0.7424\n",
      "Epoch 166/500, Loss: 0.6758, Validation Loss: 0.7421\n",
      "Epoch 167/500, Loss: 0.6669, Validation Loss: 0.7413\n",
      "Epoch 168/500, Loss: 0.6844, Validation Loss: 0.7410\n",
      "Epoch 169/500, Loss: 0.6932, Validation Loss: 0.7418\n",
      "Epoch 170/500, Loss: 0.6837, Validation Loss: 0.7422\n",
      "Epoch 171/500, Loss: 0.6786, Validation Loss: 0.7431\n",
      "Epoch 172/500, Loss: 0.6756, Validation Loss: 0.7433\n",
      "Epoch 173/500, Loss: 0.6724, Validation Loss: 0.7451\n",
      "Epoch 174/500, Loss: 0.6832, Validation Loss: 0.7463\n",
      "Epoch 175/500, Loss: 0.6944, Validation Loss: 0.7429\n",
      "Epoch 176/500, Loss: 0.6716, Validation Loss: 0.7409\n",
      "Epoch 177/500, Loss: 0.6655, Validation Loss: 0.7418\n",
      "Epoch 178/500, Loss: 0.6613, Validation Loss: 0.7427\n",
      "Epoch 179/500, Loss: 0.6652, Validation Loss: 0.7420\n",
      "Epoch 180/500, Loss: 0.6774, Validation Loss: 0.7427\n",
      "Epoch 181/500, Loss: 0.6655, Validation Loss: 0.7464\n",
      "Epoch 182/500, Loss: 0.6705, Validation Loss: 0.7474\n",
      "Epoch 183/500, Loss: 0.6715, Validation Loss: 0.7435\n",
      "Epoch 184/500, Loss: 0.6582, Validation Loss: 0.7429\n",
      "Epoch 185/500, Loss: 0.6642, Validation Loss: 0.7447\n",
      "Epoch 186/500, Loss: 0.6753, Validation Loss: 0.7446\n",
      "Epoch 187/500, Loss: 0.6609, Validation Loss: 0.7432\n",
      "Epoch 188/500, Loss: 0.6654, Validation Loss: 0.7441\n",
      "Epoch 189/500, Loss: 0.6480, Validation Loss: 0.7458\n",
      "Epoch 190/500, Loss: 0.6575, Validation Loss: 0.7479\n",
      "Epoch 191/500, Loss: 0.6717, Validation Loss: 0.7464\n",
      "Epoch 192/500, Loss: 0.6395, Validation Loss: 0.7458\n",
      "Epoch 193/500, Loss: 0.6520, Validation Loss: 0.7448\n",
      "Epoch 194/500, Loss: 0.6514, Validation Loss: 0.7441\n",
      "Epoch 195/500, Loss: 0.6455, Validation Loss: 0.7441\n",
      "Epoch 196/500, Loss: 0.6577, Validation Loss: 0.7460\n",
      "Epoch 197/500, Loss: 0.6591, Validation Loss: 0.7472\n",
      "Epoch 198/500, Loss: 0.6576, Validation Loss: 0.7468\n",
      "Epoch 199/500, Loss: 0.6397, Validation Loss: 0.7460\n",
      "Epoch 200/500, Loss: 0.6596, Validation Loss: 0.7457\n",
      "Epoch 201/500, Loss: 0.6444, Validation Loss: 0.7457\n",
      "Epoch 202/500, Loss: 0.6568, Validation Loss: 0.7436\n",
      "Epoch 203/500, Loss: 0.6497, Validation Loss: 0.7424\n",
      "Epoch 204/500, Loss: 0.6501, Validation Loss: 0.7432\n",
      "Epoch 205/500, Loss: 0.6592, Validation Loss: 0.7431\n",
      "Epoch 206/500, Loss: 0.6488, Validation Loss: 0.7428\n",
      "Epoch 207/500, Loss: 0.6596, Validation Loss: 0.7427\n",
      "Epoch 208/500, Loss: 0.6484, Validation Loss: 0.7419\n",
      "Epoch 209/500, Loss: 0.6332, Validation Loss: 0.7437\n",
      "Epoch 210/500, Loss: 0.6337, Validation Loss: 0.7476\n",
      "Epoch 211/500, Loss: 0.6411, Validation Loss: 0.7492\n",
      "Epoch 212/500, Loss: 0.6548, Validation Loss: 0.7508\n",
      "Epoch 213/500, Loss: 0.6470, Validation Loss: 0.7502\n",
      "Epoch 214/500, Loss: 0.6542, Validation Loss: 0.7501\n",
      "Epoch 215/500, Loss: 0.6380, Validation Loss: 0.7487\n",
      "Epoch 216/500, Loss: 0.6663, Validation Loss: 0.7470\n",
      "Epoch 217/500, Loss: 0.6424, Validation Loss: 0.7466\n",
      "Epoch 218/500, Loss: 0.6574, Validation Loss: 0.7484\n",
      "Epoch 219/500, Loss: 0.6390, Validation Loss: 0.7470\n",
      "Epoch 220/500, Loss: 0.6445, Validation Loss: 0.7454\n",
      "Epoch 221/500, Loss: 0.6363, Validation Loss: 0.7457\n",
      "Epoch 222/500, Loss: 0.6576, Validation Loss: 0.7468\n",
      "Epoch 223/500, Loss: 0.6205, Validation Loss: 0.7480\n",
      "Epoch 224/500, Loss: 0.6347, Validation Loss: 0.7503\n",
      "Epoch 225/500, Loss: 0.6413, Validation Loss: 0.7536\n",
      "Epoch 226/500, Loss: 0.6516, Validation Loss: 0.7560\n",
      "Epoch 227/500, Loss: 0.6370, Validation Loss: 0.7571\n",
      "Epoch 228/500, Loss: 0.6343, Validation Loss: 0.7553\n",
      "Epoch 229/500, Loss: 0.6244, Validation Loss: 0.7537\n",
      "Epoch 230/500, Loss: 0.6360, Validation Loss: 0.7518\n",
      "Epoch 231/500, Loss: 0.6418, Validation Loss: 0.7519\n",
      "Epoch 232/500, Loss: 0.6237, Validation Loss: 0.7512\n",
      "Epoch 233/500, Loss: 0.6329, Validation Loss: 0.7499\n",
      "Epoch 234/500, Loss: 0.6191, Validation Loss: 0.7498\n",
      "Epoch 235/500, Loss: 0.6247, Validation Loss: 0.7506\n",
      "Epoch 236/500, Loss: 0.6204, Validation Loss: 0.7518\n",
      "Epoch 237/500, Loss: 0.6471, Validation Loss: 0.7528\n",
      "Epoch 238/500, Loss: 0.6311, Validation Loss: 0.7543\n",
      "Epoch 239/500, Loss: 0.6276, Validation Loss: 0.7562\n",
      "Epoch 240/500, Loss: 0.6282, Validation Loss: 0.7583\n",
      "Epoch 241/500, Loss: 0.6411, Validation Loss: 0.7602\n",
      "Epoch 242/500, Loss: 0.6208, Validation Loss: 0.7602\n",
      "Epoch 243/500, Loss: 0.6059, Validation Loss: 0.7596\n",
      "Epoch 244/500, Loss: 0.6147, Validation Loss: 0.7577\n",
      "Epoch 245/500, Loss: 0.6165, Validation Loss: 0.7578\n",
      "Epoch 246/500, Loss: 0.6243, Validation Loss: 0.7576\n",
      "Epoch 247/500, Loss: 0.6315, Validation Loss: 0.7583\n",
      "Epoch 248/500, Loss: 0.6321, Validation Loss: 0.7615\n",
      "Epoch 249/500, Loss: 0.6246, Validation Loss: 0.7621\n",
      "Epoch 250/500, Loss: 0.6474, Validation Loss: 0.7595\n",
      "Epoch 251/500, Loss: 0.6416, Validation Loss: 0.7558\n",
      "Epoch 252/500, Loss: 0.6326, Validation Loss: 0.7532\n",
      "Epoch 253/500, Loss: 0.6271, Validation Loss: 0.7560\n",
      "Epoch 254/500, Loss: 0.6307, Validation Loss: 0.7596\n",
      "Epoch 255/500, Loss: 0.6269, Validation Loss: 0.7587\n",
      "Epoch 256/500, Loss: 0.6310, Validation Loss: 0.7580\n",
      "Epoch 257/500, Loss: 0.6201, Validation Loss: 0.7604\n",
      "Epoch 258/500, Loss: 0.6147, Validation Loss: 0.7607\n",
      "Epoch 259/500, Loss: 0.6382, Validation Loss: 0.7582\n",
      "Epoch 260/500, Loss: 0.6201, Validation Loss: 0.7557\n",
      "Epoch 261/500, Loss: 0.6211, Validation Loss: 0.7565\n",
      "Epoch 262/500, Loss: 0.6298, Validation Loss: 0.7591\n",
      "Epoch 263/500, Loss: 0.6236, Validation Loss: 0.7588\n",
      "Epoch 264/500, Loss: 0.6319, Validation Loss: 0.7588\n",
      "Epoch 265/500, Loss: 0.6180, Validation Loss: 0.7610\n",
      "Epoch 266/500, Loss: 0.6344, Validation Loss: 0.7629\n",
      "Epoch 267/500, Loss: 0.6304, Validation Loss: 0.7624\n",
      "Epoch 268/500, Loss: 0.6137, Validation Loss: 0.7617\n",
      "Epoch 269/500, Loss: 0.6258, Validation Loss: 0.7615\n",
      "Epoch 270/500, Loss: 0.6205, Validation Loss: 0.7598\n",
      "Epoch 271/500, Loss: 0.6062, Validation Loss: 0.7578\n",
      "Epoch 272/500, Loss: 0.6209, Validation Loss: 0.7570\n",
      "Epoch 273/500, Loss: 0.6130, Validation Loss: 0.7577\n",
      "Epoch 274/500, Loss: 0.6372, Validation Loss: 0.7589\n",
      "Epoch 275/500, Loss: 0.6125, Validation Loss: 0.7591\n",
      "Epoch 276/500, Loss: 0.6217, Validation Loss: 0.7594\n",
      "Epoch 277/500, Loss: 0.6174, Validation Loss: 0.7607\n",
      "Epoch 278/500, Loss: 0.6062, Validation Loss: 0.7621\n",
      "Epoch 279/500, Loss: 0.6121, Validation Loss: 0.7621\n",
      "Epoch 280/500, Loss: 0.6197, Validation Loss: 0.7625\n",
      "Epoch 281/500, Loss: 0.6114, Validation Loss: 0.7623\n",
      "Epoch 282/500, Loss: 0.6260, Validation Loss: 0.7598\n",
      "Epoch 283/500, Loss: 0.6004, Validation Loss: 0.7600\n",
      "Epoch 284/500, Loss: 0.6143, Validation Loss: 0.7608\n",
      "Epoch 285/500, Loss: 0.6102, Validation Loss: 0.7598\n",
      "Epoch 286/500, Loss: 0.6164, Validation Loss: 0.7597\n",
      "Epoch 287/500, Loss: 0.6250, Validation Loss: 0.7595\n",
      "Epoch 288/500, Loss: 0.6233, Validation Loss: 0.7599\n",
      "Epoch 289/500, Loss: 0.6103, Validation Loss: 0.7619\n",
      "Epoch 290/500, Loss: 0.6102, Validation Loss: 0.7630\n",
      "Epoch 291/500, Loss: 0.6129, Validation Loss: 0.7633\n",
      "Epoch 292/500, Loss: 0.6363, Validation Loss: 0.7643\n",
      "Epoch 293/500, Loss: 0.6266, Validation Loss: 0.7672\n",
      "Epoch 294/500, Loss: 0.6125, Validation Loss: 0.7718\n",
      "Epoch 295/500, Loss: 0.6190, Validation Loss: 0.7734\n",
      "Epoch 296/500, Loss: 0.6072, Validation Loss: 0.7711\n",
      "Epoch 297/500, Loss: 0.6094, Validation Loss: 0.7673\n",
      "Epoch 298/500, Loss: 0.6099, Validation Loss: 0.7646\n",
      "Epoch 299/500, Loss: 0.6239, Validation Loss: 0.7631\n",
      "Epoch 300/500, Loss: 0.5935, Validation Loss: 0.7639\n",
      "Epoch 301/500, Loss: 0.6132, Validation Loss: 0.7665\n",
      "Epoch 302/500, Loss: 0.6152, Validation Loss: 0.7684\n",
      "Epoch 303/500, Loss: 0.6253, Validation Loss: 0.7685\n",
      "Epoch 304/500, Loss: 0.6063, Validation Loss: 0.7688\n",
      "Epoch 305/500, Loss: 0.6308, Validation Loss: 0.7680\n",
      "Epoch 306/500, Loss: 0.6167, Validation Loss: 0.7628\n",
      "Epoch 307/500, Loss: 0.6023, Validation Loss: 0.7583\n",
      "Epoch 308/500, Loss: 0.6178, Validation Loss: 0.7578\n",
      "Epoch 309/500, Loss: 0.6360, Validation Loss: 0.7588\n",
      "Epoch 310/500, Loss: 0.6076, Validation Loss: 0.7620\n",
      "Epoch 311/500, Loss: 0.5950, Validation Loss: 0.7703\n",
      "Epoch 312/500, Loss: 0.6068, Validation Loss: 0.7769\n",
      "Epoch 313/500, Loss: 0.6021, Validation Loss: 0.7739\n",
      "Epoch 314/500, Loss: 0.6409, Validation Loss: 0.7684\n",
      "Epoch 315/500, Loss: 0.6093, Validation Loss: 0.7676\n",
      "Epoch 316/500, Loss: 0.6150, Validation Loss: 0.7660\n",
      "Epoch 317/500, Loss: 0.6077, Validation Loss: 0.7700\n",
      "Epoch 318/500, Loss: 0.6251, Validation Loss: 0.7711\n",
      "Epoch 319/500, Loss: 0.6080, Validation Loss: 0.7678\n",
      "Epoch 320/500, Loss: 0.6134, Validation Loss: 0.7682\n",
      "Epoch 321/500, Loss: 0.6157, Validation Loss: 0.7694\n",
      "Epoch 322/500, Loss: 0.6135, Validation Loss: 0.7696\n",
      "Epoch 323/500, Loss: 0.6029, Validation Loss: 0.7732\n",
      "Epoch 324/500, Loss: 0.6196, Validation Loss: 0.7773\n",
      "Epoch 325/500, Loss: 0.6095, Validation Loss: 0.7764\n",
      "Epoch 326/500, Loss: 0.5995, Validation Loss: 0.7740\n",
      "Epoch 327/500, Loss: 0.5942, Validation Loss: 0.7700\n",
      "Epoch 328/500, Loss: 0.6054, Validation Loss: 0.7672\n",
      "Epoch 329/500, Loss: 0.5965, Validation Loss: 0.7663\n",
      "Epoch 330/500, Loss: 0.6171, Validation Loss: 0.7679\n",
      "Epoch 331/500, Loss: 0.6082, Validation Loss: 0.7677\n",
      "Epoch 332/500, Loss: 0.5855, Validation Loss: 0.7715\n",
      "Epoch 333/500, Loss: 0.5945, Validation Loss: 0.7795\n",
      "Epoch 334/500, Loss: 0.6088, Validation Loss: 0.7831\n",
      "Epoch 335/500, Loss: 0.6306, Validation Loss: 0.7812\n",
      "Epoch 336/500, Loss: 0.6118, Validation Loss: 0.7771\n",
      "Epoch 337/500, Loss: 0.6033, Validation Loss: 0.7751\n",
      "Epoch 338/500, Loss: 0.6024, Validation Loss: 0.7731\n",
      "Epoch 339/500, Loss: 0.6106, Validation Loss: 0.7721\n",
      "Epoch 340/500, Loss: 0.6119, Validation Loss: 0.7716\n",
      "Epoch 341/500, Loss: 0.6083, Validation Loss: 0.7728\n",
      "Epoch 342/500, Loss: 0.6020, Validation Loss: 0.7773\n",
      "Epoch 343/500, Loss: 0.6140, Validation Loss: 0.7772\n",
      "Epoch 344/500, Loss: 0.6113, Validation Loss: 0.7736\n",
      "Epoch 345/500, Loss: 0.6118, Validation Loss: 0.7692\n",
      "Epoch 346/500, Loss: 0.5801, Validation Loss: 0.7677\n",
      "Epoch 347/500, Loss: 0.6064, Validation Loss: 0.7702\n",
      "Epoch 348/500, Loss: 0.5806, Validation Loss: 0.7739\n",
      "Epoch 349/500, Loss: 0.5937, Validation Loss: 0.7723\n",
      "Epoch 350/500, Loss: 0.5967, Validation Loss: 0.7689\n",
      "Epoch 351/500, Loss: 0.5955, Validation Loss: 0.7676\n",
      "Epoch 352/500, Loss: 0.5917, Validation Loss: 0.7700\n",
      "Epoch 353/500, Loss: 0.5935, Validation Loss: 0.7726\n",
      "Epoch 354/500, Loss: 0.5885, Validation Loss: 0.7738\n",
      "Epoch 355/500, Loss: 0.5861, Validation Loss: 0.7755\n",
      "Epoch 356/500, Loss: 0.5936, Validation Loss: 0.7759\n",
      "Epoch 357/500, Loss: 0.5950, Validation Loss: 0.7751\n",
      "Epoch 358/500, Loss: 0.5741, Validation Loss: 0.7749\n",
      "Epoch 359/500, Loss: 0.5902, Validation Loss: 0.7736\n",
      "Epoch 360/500, Loss: 0.5768, Validation Loss: 0.7739\n",
      "Epoch 361/500, Loss: 0.5774, Validation Loss: 0.7752\n",
      "Epoch 362/500, Loss: 0.5810, Validation Loss: 0.7751\n",
      "Epoch 363/500, Loss: 0.5963, Validation Loss: 0.7736\n",
      "Epoch 364/500, Loss: 0.6043, Validation Loss: 0.7731\n",
      "Epoch 365/500, Loss: 0.5878, Validation Loss: 0.7732\n",
      "Epoch 366/500, Loss: 0.5956, Validation Loss: 0.7757\n",
      "Epoch 367/500, Loss: 0.5934, Validation Loss: 0.7782\n",
      "Epoch 368/500, Loss: 0.5734, Validation Loss: 0.7799\n",
      "Epoch 369/500, Loss: 0.5777, Validation Loss: 0.7806\n",
      "Epoch 370/500, Loss: 0.5934, Validation Loss: 0.7813\n",
      "Epoch 371/500, Loss: 0.5861, Validation Loss: 0.7796\n",
      "Epoch 372/500, Loss: 0.5555, Validation Loss: 0.7806\n",
      "Epoch 373/500, Loss: 0.5904, Validation Loss: 0.7822\n",
      "Epoch 374/500, Loss: 0.5694, Validation Loss: 0.7813\n",
      "Epoch 375/500, Loss: 0.5760, Validation Loss: 0.7828\n",
      "Epoch 376/500, Loss: 0.6035, Validation Loss: 0.7841\n",
      "Epoch 377/500, Loss: 0.6033, Validation Loss: 0.7841\n",
      "Epoch 378/500, Loss: 0.5903, Validation Loss: 0.7868\n",
      "Epoch 379/500, Loss: 0.6092, Validation Loss: 0.7892\n",
      "Epoch 380/500, Loss: 0.5830, Validation Loss: 0.7864\n",
      "Epoch 381/500, Loss: 0.5921, Validation Loss: 0.7806\n",
      "Epoch 382/500, Loss: 0.5856, Validation Loss: 0.7784\n",
      "Epoch 383/500, Loss: 0.5858, Validation Loss: 0.7749\n",
      "Epoch 384/500, Loss: 0.6064, Validation Loss: 0.7739\n",
      "Epoch 385/500, Loss: 0.5824, Validation Loss: 0.7744\n",
      "Epoch 386/500, Loss: 0.6073, Validation Loss: 0.7758\n",
      "Epoch 387/500, Loss: 0.5723, Validation Loss: 0.7766\n",
      "Epoch 388/500, Loss: 0.6059, Validation Loss: 0.7757\n",
      "Epoch 389/500, Loss: 0.5735, Validation Loss: 0.7748\n",
      "Epoch 390/500, Loss: 0.5825, Validation Loss: 0.7763\n",
      "Epoch 391/500, Loss: 0.5919, Validation Loss: 0.7800\n",
      "Epoch 392/500, Loss: 0.5833, Validation Loss: 0.7843\n",
      "Epoch 393/500, Loss: 0.5776, Validation Loss: 0.7872\n",
      "Epoch 394/500, Loss: 0.5662, Validation Loss: 0.7894\n",
      "Epoch 395/500, Loss: 0.5912, Validation Loss: 0.7896\n",
      "Epoch 396/500, Loss: 0.5927, Validation Loss: 0.7865\n",
      "Epoch 397/500, Loss: 0.5669, Validation Loss: 0.7829\n",
      "Epoch 398/500, Loss: 0.5698, Validation Loss: 0.7795\n",
      "Epoch 399/500, Loss: 0.5670, Validation Loss: 0.7792\n",
      "Epoch 400/500, Loss: 0.5829, Validation Loss: 0.7794\n",
      "Epoch 401/500, Loss: 0.5866, Validation Loss: 0.7811\n",
      "Epoch 402/500, Loss: 0.5946, Validation Loss: 0.7799\n",
      "Epoch 403/500, Loss: 0.5718, Validation Loss: 0.7792\n",
      "Epoch 404/500, Loss: 0.5794, Validation Loss: 0.7814\n",
      "Epoch 405/500, Loss: 0.6063, Validation Loss: 0.7833\n",
      "Epoch 406/500, Loss: 0.5981, Validation Loss: 0.7850\n",
      "Epoch 407/500, Loss: 0.5833, Validation Loss: 0.7852\n",
      "Epoch 408/500, Loss: 0.5815, Validation Loss: 0.7832\n",
      "Epoch 409/500, Loss: 0.5906, Validation Loss: 0.7784\n",
      "Epoch 410/500, Loss: 0.5690, Validation Loss: 0.7760\n",
      "Epoch 411/500, Loss: 0.5759, Validation Loss: 0.7762\n",
      "Epoch 412/500, Loss: 0.5849, Validation Loss: 0.7779\n",
      "Epoch 413/500, Loss: 0.5678, Validation Loss: 0.7826\n",
      "Epoch 414/500, Loss: 0.6025, Validation Loss: 0.7845\n",
      "Epoch 415/500, Loss: 0.5799, Validation Loss: 0.7844\n",
      "Epoch 416/500, Loss: 0.5812, Validation Loss: 0.7826\n",
      "Epoch 417/500, Loss: 0.5878, Validation Loss: 0.7796\n",
      "Epoch 418/500, Loss: 0.5808, Validation Loss: 0.7797\n",
      "Epoch 419/500, Loss: 0.5839, Validation Loss: 0.7798\n",
      "Epoch 420/500, Loss: 0.5756, Validation Loss: 0.7809\n",
      "Epoch 421/500, Loss: 0.5790, Validation Loss: 0.7819\n",
      "Epoch 422/500, Loss: 0.5831, Validation Loss: 0.7844\n",
      "Epoch 423/500, Loss: 0.5852, Validation Loss: 0.7916\n",
      "Epoch 424/500, Loss: 0.5747, Validation Loss: 0.7938\n",
      "Epoch 425/500, Loss: 0.6189, Validation Loss: 0.7900\n",
      "Epoch 426/500, Loss: 0.5946, Validation Loss: 0.7823\n",
      "Epoch 427/500, Loss: 0.5678, Validation Loss: 0.7792\n",
      "Epoch 428/500, Loss: 0.5660, Validation Loss: 0.7772\n",
      "Epoch 429/500, Loss: 0.5924, Validation Loss: 0.7742\n",
      "Epoch 430/500, Loss: 0.5635, Validation Loss: 0.7726\n",
      "Epoch 431/500, Loss: 0.5892, Validation Loss: 0.7754\n",
      "Epoch 432/500, Loss: 0.5866, Validation Loss: 0.7805\n",
      "Epoch 433/500, Loss: 0.5676, Validation Loss: 0.7854\n",
      "Epoch 434/500, Loss: 0.5972, Validation Loss: 0.7868\n",
      "Epoch 435/500, Loss: 0.5939, Validation Loss: 0.7837\n",
      "Epoch 436/500, Loss: 0.6098, Validation Loss: 0.7818\n",
      "Epoch 437/500, Loss: 0.5686, Validation Loss: 0.7808\n",
      "Epoch 438/500, Loss: 0.5958, Validation Loss: 0.7802\n",
      "Epoch 439/500, Loss: 0.5941, Validation Loss: 0.7815\n",
      "Epoch 440/500, Loss: 0.5708, Validation Loss: 0.7834\n",
      "Epoch 441/500, Loss: 0.5745, Validation Loss: 0.7853\n",
      "Epoch 442/500, Loss: 0.5567, Validation Loss: 0.7866\n",
      "Epoch 443/500, Loss: 0.5758, Validation Loss: 0.7840\n",
      "Epoch 444/500, Loss: 0.5947, Validation Loss: 0.7842\n",
      "Epoch 445/500, Loss: 0.5500, Validation Loss: 0.7865\n",
      "Epoch 446/500, Loss: 0.5949, Validation Loss: 0.7857\n",
      "Epoch 447/500, Loss: 0.5678, Validation Loss: 0.7843\n",
      "Epoch 448/500, Loss: 0.5777, Validation Loss: 0.7829\n",
      "Epoch 449/500, Loss: 0.5661, Validation Loss: 0.7817\n",
      "Epoch 450/500, Loss: 0.5773, Validation Loss: 0.7805\n",
      "Epoch 451/500, Loss: 0.5930, Validation Loss: 0.7782\n",
      "Epoch 452/500, Loss: 0.5929, Validation Loss: 0.7786\n",
      "Epoch 453/500, Loss: 0.5762, Validation Loss: 0.7810\n",
      "Epoch 454/500, Loss: 0.5731, Validation Loss: 0.7829\n",
      "Epoch 455/500, Loss: 0.5715, Validation Loss: 0.7844\n",
      "Epoch 456/500, Loss: 0.5652, Validation Loss: 0.7841\n",
      "Epoch 457/500, Loss: 0.5690, Validation Loss: 0.7840\n",
      "Epoch 458/500, Loss: 0.5698, Validation Loss: 0.7833\n",
      "Epoch 459/500, Loss: 0.5813, Validation Loss: 0.7835\n",
      "Epoch 460/500, Loss: 0.5610, Validation Loss: 0.7825\n",
      "Epoch 461/500, Loss: 0.5642, Validation Loss: 0.7803\n",
      "Epoch 462/500, Loss: 0.5946, Validation Loss: 0.7777\n",
      "Epoch 463/500, Loss: 0.5964, Validation Loss: 0.7757\n",
      "Epoch 464/500, Loss: 0.5766, Validation Loss: 0.7773\n",
      "Epoch 465/500, Loss: 0.5617, Validation Loss: 0.7794\n",
      "Epoch 466/500, Loss: 0.5697, Validation Loss: 0.7799\n",
      "Epoch 467/500, Loss: 0.5778, Validation Loss: 0.7811\n",
      "Epoch 468/500, Loss: 0.5605, Validation Loss: 0.7811\n",
      "Epoch 469/500, Loss: 0.5747, Validation Loss: 0.7784\n",
      "Epoch 470/500, Loss: 0.5386, Validation Loss: 0.7793\n",
      "Epoch 471/500, Loss: 0.5553, Validation Loss: 0.7802\n",
      "Epoch 472/500, Loss: 0.5462, Validation Loss: 0.7826\n",
      "Epoch 473/500, Loss: 0.5824, Validation Loss: 0.7861\n",
      "Epoch 474/500, Loss: 0.5718, Validation Loss: 0.7885\n",
      "Epoch 475/500, Loss: 0.5798, Validation Loss: 0.7873\n",
      "Epoch 476/500, Loss: 0.5935, Validation Loss: 0.7840\n",
      "Epoch 477/500, Loss: 0.6004, Validation Loss: 0.7805\n",
      "Epoch 478/500, Loss: 0.5655, Validation Loss: 0.7799\n",
      "Epoch 479/500, Loss: 0.5910, Validation Loss: 0.7830\n",
      "Epoch 480/500, Loss: 0.5875, Validation Loss: 0.7894\n",
      "Epoch 481/500, Loss: 0.5813, Validation Loss: 0.7885\n",
      "Epoch 482/500, Loss: 0.5590, Validation Loss: 0.7862\n",
      "Epoch 483/500, Loss: 0.5724, Validation Loss: 0.7851\n",
      "Epoch 484/500, Loss: 0.5797, Validation Loss: 0.7853\n",
      "Epoch 485/500, Loss: 0.5871, Validation Loss: 0.7830\n",
      "Epoch 486/500, Loss: 0.5706, Validation Loss: 0.7869\n",
      "Epoch 487/500, Loss: 0.5899, Validation Loss: 0.7931\n",
      "Epoch 488/500, Loss: 0.5728, Validation Loss: 0.7943\n",
      "Epoch 489/500, Loss: 0.5769, Validation Loss: 0.7927\n",
      "Epoch 490/500, Loss: 0.6028, Validation Loss: 0.7908\n",
      "Epoch 491/500, Loss: 0.5693, Validation Loss: 0.7873\n",
      "Epoch 492/500, Loss: 0.5755, Validation Loss: 0.7828\n",
      "Epoch 493/500, Loss: 0.5522, Validation Loss: 0.7815\n",
      "Epoch 494/500, Loss: 0.5476, Validation Loss: 0.7814\n",
      "Epoch 495/500, Loss: 0.5808, Validation Loss: 0.7806\n",
      "Epoch 496/500, Loss: 0.5531, Validation Loss: 0.7817\n",
      "Epoch 497/500, Loss: 0.5469, Validation Loss: 0.7859\n",
      "Epoch 498/500, Loss: 0.5702, Validation Loss: 0.7918\n",
      "Epoch 499/500, Loss: 0.5618, Validation Loss: 0.7929\n",
      "Epoch 500/500, Loss: 0.5693, Validation Loss: 0.7906\n"
     ]
    }
   ],
   "source": [
    "# Define Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Model parameters\n",
    "input_dim = full_df.shape[1]\n",
    "encoding_dim = 3  #Reduced dimensionality\n",
    "model = Autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=1e-4)\n",
    "\n",
    "# Training\n",
    "epochs = 500\n",
    "batch_size = 16\n",
    "X_train_tensor = torch.tensor(train_df_scaled, dtype=torch.float32)\n",
    "\n",
    "def evaluate(model, test, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        X_test_tensor = torch.tensor(test, dtype=torch.float32)\n",
    "        outputs = model(X_test_tensor)  # Get decoded output\n",
    "        val_loss = criterion(outputs[1], X_test_tensor)  # Compare reconstructed data\n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    reconstructed = model(X_train_tensor)[1]\n",
    "    loss = criterion(reconstructed, X_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    val_loss = evaluate(model, test_df_scaled, criterion) \n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the behavioral data to lower dimensions\n",
    "with torch.no_grad():\n",
    "    X_train_reduced = model.encoder(X_train_tensor).numpy()\n",
    "    X_test_tensor = torch.tensor(test_df_scaled, dtype=torch.float32)\n",
    "    X_test_reduced = model.encoder(X_test_tensor).numpy()\n",
    "\n",
    "test_df_reduced = pd.DataFrame(\n",
    "        X_test_reduced,\n",
    "        index=test_df.index,\n",
    "        columns=[\"component_1\", \"component_2\", \"component_3\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 15:18:10,845 [ WARNING] /home/haotsung/HCP_behavioral_prediction/julearn_hcp/lib/python3.11/site-packages/julearn/prepare.py:195: RuntimeWarning: The dataframe has 80203 columns. Checking X for consistency might take a while. To skip this checks, set the config flag `disable_x_check` to `True`.\n",
      "  warn_with_log(\n",
      "\n",
      "2025-01-16 15:37:54,991 [ WARNING] /home/haotsung/HCP_behavioral_prediction/julearn_hcp/lib/python3.11/site-packages/julearn/prepare.py:471: RuntimeWarning: X has 79800 columns. Checking X_types for consistency might take a while. To skip this checks, set the config flag `disable_xtypes_check` to `True`.\n",
      "  warn_with_log(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_corr:\n",
      " 0     8.941516e-04\n",
      "1     3.442660e-03\n",
      "2     1.162774e-02\n",
      "3     3.633204e-04\n",
      "4     3.177245e-03\n",
      "5     1.165064e-02\n",
      "6     1.066333e-04\n",
      "7     1.946646e-03\n",
      "8     7.974767e-03\n",
      "9     8.957556e-03\n",
      "10    2.904341e-03\n",
      "11    3.149623e-03\n",
      "12    2.982044e-05\n",
      "13    1.769274e-03\n",
      "14    1.012621e-02\n",
      "15    4.472219e-09\n",
      "16    5.508813e-04\n",
      "17    1.977997e-02\n",
      "18    1.910536e-02\n",
      "19    2.318740e-02\n",
      "20    2.005093e-02\n",
      "21    1.342179e-04\n",
      "22    1.102300e-02\n",
      "23    6.934613e-03\n",
      "24    2.174031e-02\n",
      "25    1.848244e-02\n",
      "26    4.775650e-03\n",
      "27    2.921824e-04\n",
      "28    4.770307e-03\n",
      "29    5.983361e-04\n",
      "30    9.828886e-03\n",
      "31    6.030415e-02\n",
      "32    8.826164e-04\n",
      "33    1.536766e-03\n",
      "34    1.223135e-02\n",
      "35    9.692623e-04\n",
      "36    6.620950e-03\n",
      "37    8.303500e-05\n",
      "38    2.040788e-03\n",
      "39    5.181090e-04\n",
      "40    8.778065e-03\n",
      "41    1.272808e-02\n",
      "42    8.119800e-03\n",
      "43    4.959147e-04\n",
      "44    8.442090e-03\n",
      "45    7.363955e-03\n",
      "46    2.469838e-04\n",
      "47    1.240929e-03\n",
      "48    1.623200e-02\n",
      "49    1.305725e-04\n",
      "Name: test_r2_corr, dtype: float64\n",
      "r_corr:\n",
      " 0     0.029902\n",
      "1     0.058674\n",
      "2    -0.107832\n",
      "3     0.019061\n",
      "4    -0.056367\n",
      "5     0.107938\n",
      "6     0.010326\n",
      "7    -0.044121\n",
      "8     0.089302\n",
      "9    -0.094644\n",
      "10   -0.053892\n",
      "11   -0.056121\n",
      "12   -0.005461\n",
      "13    0.042063\n",
      "14    0.100629\n",
      "15    0.000067\n",
      "16    0.023471\n",
      "17   -0.140641\n",
      "18    0.138222\n",
      "19    0.152274\n",
      "20    0.141601\n",
      "21    0.011585\n",
      "22    0.104990\n",
      "23   -0.083274\n",
      "24   -0.147446\n",
      "25   -0.135950\n",
      "26    0.069106\n",
      "27   -0.017093\n",
      "28    0.069067\n",
      "29   -0.024461\n",
      "30    0.099141\n",
      "31   -0.245569\n",
      "32   -0.029709\n",
      "33   -0.039202\n",
      "34   -0.110595\n",
      "35    0.031133\n",
      "36    0.081369\n",
      "37    0.009112\n",
      "38    0.045175\n",
      "39   -0.022762\n",
      "40    0.093691\n",
      "41   -0.112819\n",
      "42    0.090110\n",
      "43    0.022269\n",
      "44   -0.091881\n",
      "45   -0.085813\n",
      "46   -0.015716\n",
      "47    0.035227\n",
      "48    0.127405\n",
      "49    0.011427\n",
      "Name: test_r_corr, dtype: float64\n",
      "Scores with best hyperparameter: 0.0018593777741319237\n"
     ]
    }
   ],
   "source": [
    "# %% Feature generation\n",
    "storage = HDF5FeatureStorage(\"features.hdf5\")\n",
    "df_features = storage.read_df('BOLD_Schaefer400x17_functional_connectivity')\n",
    "\n",
    "# Group by 'subject' and calculate the mean across REST1/LR, REST1/RL, REST2/LR, REST2/RL \n",
    "df_features = df_features.groupby('subject').mean()\n",
    "\n",
    "# merge the dataframe\n",
    "df_features.index.name= \"Subject\"\n",
    "df_data = df_features.join(test_df_reduced, how=\"inner\")\n",
    "\n",
    "# %%\n",
    "# Regression model training\n",
    "# Step 1: Seperate features (X) and targets (y)\n",
    "X = [x for x in df_data.columns if \"~\" in x]\n",
    "X = [x for x in X if x.split(\"~\")[0] != x.split(\"~\")[1]]\n",
    "y = \"component_2\"\n",
    "X_types = {\n",
    "    \"features\":[\".*~.*\"],\n",
    "} \n",
    "# %%\n",
    "# Create the pipeline that will be used to predict the target.\n",
    "kernel_ridge_model = KernelRidge()\n",
    "creator = PipelineCreator(problem_type=\"regression\",apply_to=\"features\")\n",
    "creator.add(\"zscore\")\n",
    "creator.add(\n",
    "    kernel_ridge_model, \n",
    "    alpha=10, \n",
    "    kernel='linear', \n",
    "    )\n",
    "# %%\n",
    "# Evaluate the model within the cross validation.\n",
    "rkf = RepeatedKFold(n_splits=10,n_repeats=5,random_state=42)\n",
    "scores_tuned, model_tuned = run_cross_validation(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    X_types=X_types,\n",
    "    data=df_data,\n",
    "    model=creator, \n",
    "    return_estimator=\"all\",\n",
    "    cv=rkf,\n",
    "    scoring= ['r2_corr','r_corr'],\n",
    "    n_jobs = -1,\n",
    ")\n",
    "print(\"r2_corr:\\n\",scores_tuned[\"test_r2_corr\"])\n",
    "print(\"r_corr:\\n\",scores_tuned[\"test_r_corr\"])\n",
    "print(f\"Scores with best hyperparameter: {scores_tuned['test_r_corr'].mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julearn_hcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
