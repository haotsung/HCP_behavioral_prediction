{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "from junifer.storage import HDF5FeatureStorage\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from julearn import run_cross_validation\n",
    "from julearn.pipeline import PipelineCreator\n",
    "from julearn.utils import configure_logging\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from julearn.config import set_config\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80 specific subjects same as the paper\n",
    "train_subs = pd.read_csv(\n",
    "    \"MMP_HCP_80_subs_componentscoreestimation.txt\", header=None\n",
    ").values.flatten().astype(str)  # 80 subjects\n",
    "\n",
    "# 753 specific subject for main analysis same as the paper\n",
    "test_subs = pd.read_csv(\n",
    "    \"MMP_HCP_753_subs.txt\", header=None\n",
    ").values.flatten().astype(str)  # 753 subjects\n",
    "\n",
    "columns= ['PicSeq_Unadj','CardSort_Unadj','Flanker_Unadj','PMAT24_A_CR','ReadEng_Unadj','PicVocab_Unadj','ProcSpeed_Unadj','VSPLOT_TC','SCPT_SEN','SCPT_SPEC','IWRD_TOT','ListSort_Unadj','MMSE_Score',\n",
    "                     'PSQI_Score','Endurance_Unadj','Dexterity_Unadj','Strength_Unadj','Odor_Unadj','PainInterf_Tscore','Taste_Unadj','Mars_Final','Emotion_Task_Face_Acc','Language_Task_Math_Avg_Difficulty_Level',\n",
    "                     'Language_Task_Story_Avg_Difficulty_Level','Relational_Task_Acc','Social_Task_Perc_Random','Social_Task_Perc_TOM','WM_Task_Acc','NEOFAC_A','NEOFAC_O','NEOFAC_C','NEOFAC_N','NEOFAC_E','ER40_CR','ER40ANG','ER40FEAR',\n",
    "                     'ER40HAP','ER40NOE','ER40SAD','AngAffect_Unadj','AngHostil_Unadj','AngAggr_Unadj','FearAffect_Unadj','FearSomat_Unadj','Sadness_Unadj','LifeSatisf_Unadj','MeanPurp_Unadj','PosAffect_Unadj','Friendship_Unadj',\n",
    "                     'Loneliness_Unadj','PercHostil_Unadj','PercReject_Unadj','EmotSupp_Unadj','InstruSupp_Unadj','PercStress_Unadj','SelfEff_Unadj','DDisc_AUC_40K','GaitSpeed_Comp']\n",
    "# Load the dataset\n",
    "full_df = pd.read_csv(\"Behavioral_Data\", index_col=\"Subject\")[columns]\n",
    "full_df.index = full_df.index.astype(str)\n",
    "test_df = full_df.loc[test_subs]\n",
    "train_df = full_df.loc[train_subs]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "imputer = IterativeImputer(max_iter=20, random_state=0)\n",
    "train_df_imputed = imputer.fit_transform(train_df)\n",
    "train_df_scaled = scaler.fit_transform(train_df_imputed)\n",
    "test_df_imputed = imputer.transform(test_df)\n",
    "test_df_scaled = scaler.transform(test_df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 1.0006, Validation Loss: 1.0059\n",
      "Epoch 2/500, Loss: 1.1268, Validation Loss: 0.8953\n",
      "Epoch 3/500, Loss: 1.0016, Validation Loss: 0.9000\n",
      "Epoch 4/500, Loss: 1.0024, Validation Loss: 0.8999\n",
      "Epoch 5/500, Loss: 1.0022, Validation Loss: 0.8982\n",
      "Epoch 6/500, Loss: 1.0009, Validation Loss: 0.8964\n",
      "Epoch 7/500, Loss: 0.9996, Validation Loss: 0.8843\n",
      "Epoch 8/500, Loss: 0.9908, Validation Loss: 0.8363\n",
      "Epoch 9/500, Loss: 0.9567, Validation Loss: 0.8533\n",
      "Epoch 10/500, Loss: 0.9560, Validation Loss: 0.8348\n",
      "Epoch 11/500, Loss: 0.9313, Validation Loss: 0.9927\n",
      "Epoch 12/500, Loss: 1.0591, Validation Loss: 0.8567\n",
      "Epoch 13/500, Loss: 0.9379, Validation Loss: 0.9292\n",
      "Epoch 14/500, Loss: 0.9927, Validation Loss: 0.9533\n",
      "Epoch 15/500, Loss: 1.0103, Validation Loss: 0.9457\n",
      "Epoch 16/500, Loss: 1.0072, Validation Loss: 0.9265\n",
      "Epoch 17/500, Loss: 0.9970, Validation Loss: 0.9079\n",
      "Epoch 18/500, Loss: 0.9879, Validation Loss: 0.8926\n",
      "Epoch 19/500, Loss: 0.9806, Validation Loss: 0.8817\n",
      "Epoch 20/500, Loss: 0.9748, Validation Loss: 0.8730\n",
      "Epoch 21/500, Loss: 0.9674, Validation Loss: 0.8610\n",
      "Epoch 22/500, Loss: 0.9523, Validation Loss: 0.8440\n",
      "Epoch 23/500, Loss: 0.9293, Validation Loss: 0.8247\n",
      "Epoch 24/500, Loss: 0.9021, Validation Loss: 0.8099\n",
      "Epoch 25/500, Loss: 0.8781, Validation Loss: 0.8184\n",
      "Epoch 26/500, Loss: 0.8814, Validation Loss: 0.8079\n",
      "Epoch 27/500, Loss: 0.8703, Validation Loss: 0.7984\n",
      "Epoch 28/500, Loss: 0.8556, Validation Loss: 0.8002\n",
      "Epoch 29/500, Loss: 0.8537, Validation Loss: 0.8033\n",
      "Epoch 30/500, Loss: 0.8518, Validation Loss: 0.8020\n",
      "Epoch 31/500, Loss: 0.8425, Validation Loss: 0.8018\n",
      "Epoch 32/500, Loss: 0.8346, Validation Loss: 0.8024\n",
      "Epoch 33/500, Loss: 0.8268, Validation Loss: 0.7950\n",
      "Epoch 34/500, Loss: 0.8153, Validation Loss: 0.7937\n",
      "Epoch 35/500, Loss: 0.8118, Validation Loss: 0.7911\n",
      "Epoch 36/500, Loss: 0.8045, Validation Loss: 0.7927\n",
      "Epoch 37/500, Loss: 0.7968, Validation Loss: 0.7990\n",
      "Epoch 38/500, Loss: 0.7944, Validation Loss: 0.7971\n",
      "Epoch 39/500, Loss: 0.8044, Validation Loss: 0.8617\n",
      "Epoch 40/500, Loss: 0.8513, Validation Loss: 0.8065\n",
      "Epoch 41/500, Loss: 0.8226, Validation Loss: 0.7948\n",
      "Epoch 42/500, Loss: 0.8173, Validation Loss: 0.7881\n",
      "Epoch 43/500, Loss: 0.7986, Validation Loss: 0.7962\n",
      "Epoch 44/500, Loss: 0.8020, Validation Loss: 0.7995\n",
      "Epoch 45/500, Loss: 0.7820, Validation Loss: 0.8097\n",
      "Epoch 46/500, Loss: 0.7930, Validation Loss: 0.7932\n",
      "Epoch 47/500, Loss: 0.7857, Validation Loss: 0.7911\n",
      "Epoch 48/500, Loss: 0.7915, Validation Loss: 0.7757\n",
      "Epoch 49/500, Loss: 0.7777, Validation Loss: 0.7891\n",
      "Epoch 50/500, Loss: 0.7819, Validation Loss: 0.7893\n",
      "Epoch 51/500, Loss: 0.7625, Validation Loss: 0.8127\n",
      "Epoch 52/500, Loss: 0.7745, Validation Loss: 0.8015\n",
      "Epoch 53/500, Loss: 0.7587, Validation Loss: 0.7943\n",
      "Epoch 54/500, Loss: 0.7593, Validation Loss: 0.7856\n",
      "Epoch 55/500, Loss: 0.7528, Validation Loss: 0.7911\n",
      "Epoch 56/500, Loss: 0.7538, Validation Loss: 0.7908\n",
      "Epoch 57/500, Loss: 0.7430, Validation Loss: 0.7981\n",
      "Epoch 58/500, Loss: 0.7473, Validation Loss: 0.8016\n",
      "Epoch 59/500, Loss: 0.7376, Validation Loss: 0.7985\n",
      "Epoch 60/500, Loss: 0.7335, Validation Loss: 0.7933\n",
      "Epoch 61/500, Loss: 0.7315, Validation Loss: 0.7950\n",
      "Epoch 62/500, Loss: 0.7268, Validation Loss: 0.8062\n",
      "Epoch 63/500, Loss: 0.7246, Validation Loss: 0.8057\n",
      "Epoch 64/500, Loss: 0.7177, Validation Loss: 0.8074\n",
      "Epoch 65/500, Loss: 0.7180, Validation Loss: 0.8110\n",
      "Epoch 66/500, Loss: 0.7131, Validation Loss: 0.8088\n",
      "Epoch 67/500, Loss: 0.7081, Validation Loss: 0.8082\n",
      "Epoch 68/500, Loss: 0.7087, Validation Loss: 0.8180\n",
      "Epoch 69/500, Loss: 0.7052, Validation Loss: 0.8153\n",
      "Epoch 70/500, Loss: 0.7001, Validation Loss: 0.8171\n",
      "Epoch 71/500, Loss: 0.6984, Validation Loss: 0.8291\n",
      "Epoch 72/500, Loss: 0.6997, Validation Loss: 0.8176\n",
      "Epoch 73/500, Loss: 0.6960, Validation Loss: 0.8239\n",
      "Epoch 74/500, Loss: 0.6920, Validation Loss: 0.8316\n",
      "Epoch 75/500, Loss: 0.6915, Validation Loss: 0.8267\n",
      "Epoch 76/500, Loss: 0.6977, Validation Loss: 0.8618\n",
      "Epoch 77/500, Loss: 0.7131, Validation Loss: 0.8272\n",
      "Epoch 78/500, Loss: 0.7111, Validation Loss: 0.8373\n",
      "Epoch 79/500, Loss: 0.6931, Validation Loss: 0.8458\n",
      "Epoch 80/500, Loss: 0.6993, Validation Loss: 0.8284\n",
      "Epoch 81/500, Loss: 0.7021, Validation Loss: 0.8468\n",
      "Epoch 82/500, Loss: 0.7072, Validation Loss: 0.8428\n",
      "Epoch 83/500, Loss: 0.6964, Validation Loss: 0.8412\n",
      "Epoch 84/500, Loss: 0.7065, Validation Loss: 0.8453\n",
      "Epoch 85/500, Loss: 0.6965, Validation Loss: 0.8334\n",
      "Epoch 86/500, Loss: 0.6882, Validation Loss: 0.8286\n",
      "Epoch 87/500, Loss: 0.6983, Validation Loss: 0.8371\n",
      "Epoch 88/500, Loss: 0.6920, Validation Loss: 0.8408\n",
      "Epoch 89/500, Loss: 0.6869, Validation Loss: 0.8378\n",
      "Epoch 90/500, Loss: 0.6994, Validation Loss: 0.8484\n",
      "Epoch 91/500, Loss: 0.6992, Validation Loss: 0.8352\n",
      "Epoch 92/500, Loss: 0.6938, Validation Loss: 0.8405\n",
      "Epoch 93/500, Loss: 0.6925, Validation Loss: 0.8405\n",
      "Epoch 94/500, Loss: 0.6927, Validation Loss: 0.8792\n",
      "Epoch 95/500, Loss: 0.7148, Validation Loss: 0.8737\n",
      "Epoch 96/500, Loss: 0.7868, Validation Loss: 0.9562\n",
      "Epoch 97/500, Loss: 0.8250, Validation Loss: 0.8442\n",
      "Epoch 98/500, Loss: 0.7102, Validation Loss: 0.8632\n",
      "Epoch 99/500, Loss: 0.7800, Validation Loss: 0.8345\n",
      "Epoch 100/500, Loss: 0.7153, Validation Loss: 0.8568\n",
      "Epoch 101/500, Loss: 0.7387, Validation Loss: 0.8516\n",
      "Epoch 102/500, Loss: 0.7272, Validation Loss: 0.8179\n",
      "Epoch 103/500, Loss: 0.7078, Validation Loss: 0.8140\n",
      "Epoch 104/500, Loss: 0.7098, Validation Loss: 0.8091\n",
      "Epoch 105/500, Loss: 0.7067, Validation Loss: 0.8192\n",
      "Epoch 106/500, Loss: 0.7120, Validation Loss: 0.8181\n",
      "Epoch 107/500, Loss: 0.6931, Validation Loss: 0.8221\n",
      "Epoch 108/500, Loss: 0.7047, Validation Loss: 0.8171\n",
      "Epoch 109/500, Loss: 0.6838, Validation Loss: 0.8200\n",
      "Epoch 110/500, Loss: 0.6890, Validation Loss: 0.8179\n",
      "Epoch 111/500, Loss: 0.6893, Validation Loss: 0.8180\n",
      "Epoch 112/500, Loss: 0.6867, Validation Loss: 0.8200\n",
      "Epoch 113/500, Loss: 0.6816, Validation Loss: 0.8249\n",
      "Epoch 114/500, Loss: 0.6780, Validation Loss: 0.8300\n",
      "Epoch 115/500, Loss: 0.6794, Validation Loss: 0.8288\n",
      "Epoch 116/500, Loss: 0.6748, Validation Loss: 0.8308\n",
      "Epoch 117/500, Loss: 0.6731, Validation Loss: 0.8322\n",
      "Epoch 118/500, Loss: 0.6711, Validation Loss: 0.8347\n",
      "Epoch 119/500, Loss: 0.6702, Validation Loss: 0.8355\n",
      "Epoch 120/500, Loss: 0.6675, Validation Loss: 0.8375\n",
      "Epoch 121/500, Loss: 0.6680, Validation Loss: 0.8421\n",
      "Epoch 122/500, Loss: 0.6667, Validation Loss: 0.8418\n",
      "Epoch 123/500, Loss: 0.6639, Validation Loss: 0.8421\n",
      "Epoch 124/500, Loss: 0.6645, Validation Loss: 0.8443\n",
      "Epoch 125/500, Loss: 0.6615, Validation Loss: 0.8487\n",
      "Epoch 126/500, Loss: 0.6626, Validation Loss: 0.8454\n",
      "Epoch 127/500, Loss: 0.6600, Validation Loss: 0.8455\n",
      "Epoch 128/500, Loss: 0.6585, Validation Loss: 0.8490\n",
      "Epoch 129/500, Loss: 0.6578, Validation Loss: 0.8478\n",
      "Epoch 130/500, Loss: 0.6564, Validation Loss: 0.8481\n",
      "Epoch 131/500, Loss: 0.6568, Validation Loss: 0.8520\n",
      "Epoch 132/500, Loss: 0.6544, Validation Loss: 0.8524\n",
      "Epoch 133/500, Loss: 0.6531, Validation Loss: 0.8511\n",
      "Epoch 134/500, Loss: 0.6528, Validation Loss: 0.8540\n",
      "Epoch 135/500, Loss: 0.6519, Validation Loss: 0.8526\n",
      "Epoch 136/500, Loss: 0.6504, Validation Loss: 0.8527\n",
      "Epoch 137/500, Loss: 0.6497, Validation Loss: 0.8575\n",
      "Epoch 138/500, Loss: 0.6498, Validation Loss: 0.8549\n",
      "Epoch 139/500, Loss: 0.6480, Validation Loss: 0.8571\n",
      "Epoch 140/500, Loss: 0.6464, Validation Loss: 0.8578\n",
      "Epoch 141/500, Loss: 0.6456, Validation Loss: 0.8579\n",
      "Epoch 142/500, Loss: 0.6447, Validation Loss: 0.8623\n",
      "Epoch 143/500, Loss: 0.6446, Validation Loss: 0.8603\n",
      "Epoch 144/500, Loss: 0.6439, Validation Loss: 0.8652\n",
      "Epoch 145/500, Loss: 0.6437, Validation Loss: 0.8622\n",
      "Epoch 146/500, Loss: 0.6425, Validation Loss: 0.8661\n",
      "Epoch 147/500, Loss: 0.6404, Validation Loss: 0.8639\n",
      "Epoch 148/500, Loss: 0.6398, Validation Loss: 0.8665\n",
      "Epoch 149/500, Loss: 0.6388, Validation Loss: 0.8634\n",
      "Epoch 150/500, Loss: 0.6394, Validation Loss: 0.8705\n",
      "Epoch 151/500, Loss: 0.6393, Validation Loss: 0.8640\n",
      "Epoch 152/500, Loss: 0.6428, Validation Loss: 0.8773\n",
      "Epoch 153/500, Loss: 0.6476, Validation Loss: 0.8642\n",
      "Epoch 154/500, Loss: 0.6469, Validation Loss: 0.8696\n",
      "Epoch 155/500, Loss: 0.6359, Validation Loss: 0.8697\n",
      "Epoch 156/500, Loss: 0.6336, Validation Loss: 0.8644\n",
      "Epoch 157/500, Loss: 0.6399, Validation Loss: 0.8811\n",
      "Epoch 158/500, Loss: 0.6503, Validation Loss: 0.8628\n",
      "Epoch 159/500, Loss: 0.6439, Validation Loss: 0.8638\n",
      "Epoch 160/500, Loss: 0.6309, Validation Loss: 0.8763\n",
      "Epoch 161/500, Loss: 0.6419, Validation Loss: 0.8594\n",
      "Epoch 162/500, Loss: 0.6463, Validation Loss: 0.8708\n",
      "Epoch 163/500, Loss: 0.6431, Validation Loss: 0.8660\n",
      "Epoch 164/500, Loss: 0.6342, Validation Loss: 0.8627\n",
      "Epoch 165/500, Loss: 0.6401, Validation Loss: 0.8833\n",
      "Epoch 166/500, Loss: 0.6447, Validation Loss: 0.8611\n",
      "Epoch 167/500, Loss: 0.6323, Validation Loss: 0.8612\n",
      "Epoch 168/500, Loss: 0.6285, Validation Loss: 0.8744\n",
      "Epoch 169/500, Loss: 0.6351, Validation Loss: 0.8648\n",
      "Epoch 170/500, Loss: 0.6306, Validation Loss: 0.8715\n",
      "Epoch 171/500, Loss: 0.6245, Validation Loss: 0.8694\n",
      "Epoch 172/500, Loss: 0.6232, Validation Loss: 0.8640\n",
      "Epoch 173/500, Loss: 0.6266, Validation Loss: 0.8743\n",
      "Epoch 174/500, Loss: 0.6250, Validation Loss: 0.8678\n",
      "Epoch 175/500, Loss: 0.6226, Validation Loss: 0.8708\n",
      "Epoch 176/500, Loss: 0.6194, Validation Loss: 0.8736\n",
      "Epoch 177/500, Loss: 0.6211, Validation Loss: 0.8702\n",
      "Epoch 178/500, Loss: 0.6203, Validation Loss: 0.8798\n",
      "Epoch 179/500, Loss: 0.6217, Validation Loss: 0.8714\n",
      "Epoch 180/500, Loss: 0.6211, Validation Loss: 0.8817\n",
      "Epoch 181/500, Loss: 0.6235, Validation Loss: 0.8727\n",
      "Epoch 182/500, Loss: 0.6267, Validation Loss: 0.8865\n",
      "Epoch 183/500, Loss: 0.6273, Validation Loss: 0.8694\n",
      "Epoch 184/500, Loss: 0.6263, Validation Loss: 0.8809\n",
      "Epoch 185/500, Loss: 0.6221, Validation Loss: 0.8748\n",
      "Epoch 186/500, Loss: 0.6195, Validation Loss: 0.8788\n",
      "Epoch 187/500, Loss: 0.6128, Validation Loss: 0.8804\n",
      "Epoch 188/500, Loss: 0.6151, Validation Loss: 0.8803\n",
      "Epoch 189/500, Loss: 0.6127, Validation Loss: 0.8910\n",
      "Epoch 190/500, Loss: 0.6188, Validation Loss: 0.8787\n",
      "Epoch 191/500, Loss: 0.6198, Validation Loss: 0.8915\n",
      "Epoch 192/500, Loss: 0.6258, Validation Loss: 0.8756\n",
      "Epoch 193/500, Loss: 0.6338, Validation Loss: 0.8952\n",
      "Epoch 194/500, Loss: 0.6395, Validation Loss: 0.8708\n",
      "Epoch 195/500, Loss: 0.6271, Validation Loss: 0.8732\n",
      "Epoch 196/500, Loss: 0.6125, Validation Loss: 0.8925\n",
      "Epoch 197/500, Loss: 0.6245, Validation Loss: 0.8718\n",
      "Epoch 198/500, Loss: 0.6245, Validation Loss: 0.8759\n",
      "Epoch 199/500, Loss: 0.6237, Validation Loss: 0.8769\n",
      "Epoch 200/500, Loss: 0.6196, Validation Loss: 0.8798\n",
      "Epoch 201/500, Loss: 0.6092, Validation Loss: 0.8855\n",
      "Epoch 202/500, Loss: 0.6158, Validation Loss: 0.8746\n",
      "Epoch 203/500, Loss: 0.6177, Validation Loss: 0.8876\n",
      "Epoch 204/500, Loss: 0.6194, Validation Loss: 0.8781\n",
      "Epoch 205/500, Loss: 0.6182, Validation Loss: 0.8820\n",
      "Epoch 206/500, Loss: 0.6087, Validation Loss: 0.8858\n",
      "Epoch 207/500, Loss: 0.6020, Validation Loss: 0.8874\n",
      "Epoch 208/500, Loss: 0.6113, Validation Loss: 0.9009\n",
      "Epoch 209/500, Loss: 0.6218, Validation Loss: 0.8862\n",
      "Epoch 210/500, Loss: 0.6399, Validation Loss: 0.8970\n",
      "Epoch 211/500, Loss: 0.6246, Validation Loss: 0.8862\n",
      "Epoch 212/500, Loss: 0.6019, Validation Loss: 0.8857\n",
      "Epoch 213/500, Loss: 0.6210, Validation Loss: 0.9086\n",
      "Epoch 214/500, Loss: 0.6453, Validation Loss: 0.8759\n",
      "Epoch 215/500, Loss: 0.6221, Validation Loss: 0.8775\n",
      "Epoch 216/500, Loss: 0.6052, Validation Loss: 0.8981\n",
      "Epoch 217/500, Loss: 0.6237, Validation Loss: 0.8852\n",
      "Epoch 218/500, Loss: 0.6120, Validation Loss: 0.8833\n",
      "Epoch 219/500, Loss: 0.5988, Validation Loss: 0.8914\n",
      "Epoch 220/500, Loss: 0.6096, Validation Loss: 0.8811\n",
      "Epoch 221/500, Loss: 0.6080, Validation Loss: 0.8850\n",
      "Epoch 222/500, Loss: 0.5952, Validation Loss: 0.8929\n",
      "Epoch 223/500, Loss: 0.5970, Validation Loss: 0.8931\n",
      "Epoch 224/500, Loss: 0.6020, Validation Loss: 0.9018\n",
      "Epoch 225/500, Loss: 0.5945, Validation Loss: 0.8906\n",
      "Epoch 226/500, Loss: 0.5920, Validation Loss: 0.8930\n",
      "Epoch 227/500, Loss: 0.5907, Validation Loss: 0.9090\n",
      "Epoch 228/500, Loss: 0.5959, Validation Loss: 0.8994\n",
      "Epoch 229/500, Loss: 0.5987, Validation Loss: 0.9079\n",
      "Epoch 230/500, Loss: 0.5983, Validation Loss: 0.8942\n",
      "Epoch 231/500, Loss: 0.5920, Validation Loss: 0.9024\n",
      "Epoch 232/500, Loss: 0.5870, Validation Loss: 0.9043\n",
      "Epoch 233/500, Loss: 0.5858, Validation Loss: 0.8999\n",
      "Epoch 234/500, Loss: 0.5888, Validation Loss: 0.9139\n",
      "Epoch 235/500, Loss: 0.5971, Validation Loss: 0.8900\n",
      "Epoch 236/500, Loss: 0.6069, Validation Loss: 0.9005\n",
      "Epoch 237/500, Loss: 0.6116, Validation Loss: 0.8919\n",
      "Epoch 238/500, Loss: 0.5956, Validation Loss: 0.9077\n",
      "Epoch 239/500, Loss: 0.5859, Validation Loss: 0.9041\n",
      "Epoch 240/500, Loss: 0.5848, Validation Loss: 0.8986\n",
      "Epoch 241/500, Loss: 0.5909, Validation Loss: 0.9145\n",
      "Epoch 242/500, Loss: 0.6023, Validation Loss: 0.8907\n",
      "Epoch 243/500, Loss: 0.5990, Validation Loss: 0.8997\n",
      "Epoch 244/500, Loss: 0.5889, Validation Loss: 0.8989\n",
      "Epoch 245/500, Loss: 0.5790, Validation Loss: 0.9026\n",
      "Epoch 246/500, Loss: 0.5809, Validation Loss: 0.9120\n",
      "Epoch 247/500, Loss: 0.5860, Validation Loss: 0.9015\n",
      "Epoch 248/500, Loss: 0.5958, Validation Loss: 0.9211\n",
      "Epoch 249/500, Loss: 0.6078, Validation Loss: 0.8937\n",
      "Epoch 250/500, Loss: 0.5994, Validation Loss: 0.8971\n",
      "Epoch 251/500, Loss: 0.5832, Validation Loss: 0.9031\n",
      "Epoch 252/500, Loss: 0.5848, Validation Loss: 0.8967\n",
      "Epoch 253/500, Loss: 0.5986, Validation Loss: 0.9150\n",
      "Epoch 254/500, Loss: 0.5952, Validation Loss: 0.9040\n",
      "Epoch 255/500, Loss: 0.5787, Validation Loss: 0.9047\n",
      "Epoch 256/500, Loss: 0.5724, Validation Loss: 0.9131\n",
      "Epoch 257/500, Loss: 0.5797, Validation Loss: 0.9036\n",
      "Epoch 258/500, Loss: 0.5826, Validation Loss: 0.9157\n",
      "Epoch 259/500, Loss: 0.5785, Validation Loss: 0.9074\n",
      "Epoch 260/500, Loss: 0.5744, Validation Loss: 0.9066\n",
      "Epoch 261/500, Loss: 0.5667, Validation Loss: 0.9059\n",
      "Epoch 262/500, Loss: 0.5655, Validation Loss: 0.9057\n",
      "Epoch 263/500, Loss: 0.5664, Validation Loss: 0.9209\n",
      "Epoch 264/500, Loss: 0.5776, Validation Loss: 0.9066\n",
      "Epoch 265/500, Loss: 0.5993, Validation Loss: 0.9264\n",
      "Epoch 266/500, Loss: 0.6077, Validation Loss: 0.8996\n",
      "Epoch 267/500, Loss: 0.6059, Validation Loss: 0.9138\n",
      "Epoch 268/500, Loss: 0.5792, Validation Loss: 0.9063\n",
      "Epoch 269/500, Loss: 0.5640, Validation Loss: 0.9112\n",
      "Epoch 270/500, Loss: 0.5753, Validation Loss: 0.9253\n",
      "Epoch 271/500, Loss: 0.5898, Validation Loss: 0.8936\n",
      "Epoch 272/500, Loss: 0.5929, Validation Loss: 0.9001\n",
      "Epoch 273/500, Loss: 0.5770, Validation Loss: 0.9071\n",
      "Epoch 274/500, Loss: 0.5642, Validation Loss: 0.9127\n",
      "Epoch 275/500, Loss: 0.5728, Validation Loss: 0.9248\n",
      "Epoch 276/500, Loss: 0.5793, Validation Loss: 0.9010\n",
      "Epoch 277/500, Loss: 0.5761, Validation Loss: 0.9152\n",
      "Epoch 278/500, Loss: 0.5683, Validation Loss: 0.9066\n",
      "Epoch 279/500, Loss: 0.5593, Validation Loss: 0.9076\n",
      "Epoch 280/500, Loss: 0.5577, Validation Loss: 0.9189\n",
      "Epoch 281/500, Loss: 0.5626, Validation Loss: 0.9035\n",
      "Epoch 282/500, Loss: 0.5663, Validation Loss: 0.9121\n",
      "Epoch 283/500, Loss: 0.5642, Validation Loss: 0.9092\n",
      "Epoch 284/500, Loss: 0.5609, Validation Loss: 0.9138\n",
      "Epoch 285/500, Loss: 0.5478, Validation Loss: 0.9187\n",
      "Epoch 286/500, Loss: 0.5551, Validation Loss: 0.9158\n",
      "Epoch 287/500, Loss: 0.5648, Validation Loss: 0.9347\n",
      "Epoch 288/500, Loss: 0.5828, Validation Loss: 0.9033\n",
      "Epoch 289/500, Loss: 0.5827, Validation Loss: 0.9128\n",
      "Epoch 290/500, Loss: 0.5550, Validation Loss: 0.9200\n",
      "Epoch 291/500, Loss: 0.5479, Validation Loss: 0.9206\n",
      "Epoch 292/500, Loss: 0.5771, Validation Loss: 0.9412\n",
      "Epoch 293/500, Loss: 0.6059, Validation Loss: 0.9169\n",
      "Epoch 294/500, Loss: 0.6233, Validation Loss: 0.9060\n",
      "Epoch 295/500, Loss: 0.5639, Validation Loss: 0.9144\n",
      "Epoch 296/500, Loss: 0.5792, Validation Loss: 0.9163\n",
      "Epoch 297/500, Loss: 0.6210, Validation Loss: 0.9387\n",
      "Epoch 298/500, Loss: 0.5862, Validation Loss: 0.9160\n",
      "Epoch 299/500, Loss: 0.5528, Validation Loss: 0.9067\n",
      "Epoch 300/500, Loss: 0.5881, Validation Loss: 0.9072\n",
      "Epoch 301/500, Loss: 0.5554, Validation Loss: 0.9118\n",
      "Epoch 302/500, Loss: 0.5530, Validation Loss: 0.9091\n",
      "Epoch 303/500, Loss: 0.5856, Validation Loss: 0.9248\n",
      "Epoch 304/500, Loss: 0.5758, Validation Loss: 0.9093\n",
      "Epoch 305/500, Loss: 0.5667, Validation Loss: 0.9019\n",
      "Epoch 306/500, Loss: 0.5566, Validation Loss: 0.9159\n",
      "Epoch 307/500, Loss: 0.5631, Validation Loss: 0.9144\n",
      "Epoch 308/500, Loss: 0.5446, Validation Loss: 0.9314\n",
      "Epoch 309/500, Loss: 0.5488, Validation Loss: 0.9371\n",
      "Epoch 310/500, Loss: 0.5578, Validation Loss: 0.9060\n",
      "Epoch 311/500, Loss: 0.5763, Validation Loss: 0.9066\n",
      "Epoch 312/500, Loss: 0.5502, Validation Loss: 0.9224\n",
      "Epoch 313/500, Loss: 0.5515, Validation Loss: 0.9222\n",
      "Epoch 314/500, Loss: 0.5643, Validation Loss: 0.9304\n",
      "Epoch 315/500, Loss: 0.5396, Validation Loss: 0.9264\n",
      "Epoch 316/500, Loss: 0.5460, Validation Loss: 0.9146\n",
      "Epoch 317/500, Loss: 0.5576, Validation Loss: 0.9240\n",
      "Epoch 318/500, Loss: 0.5351, Validation Loss: 0.9365\n",
      "Epoch 319/500, Loss: 0.5487, Validation Loss: 0.9174\n",
      "Epoch 320/500, Loss: 0.5801, Validation Loss: 0.9273\n",
      "Epoch 321/500, Loss: 0.5575, Validation Loss: 0.9224\n",
      "Epoch 322/500, Loss: 0.5399, Validation Loss: 0.9254\n",
      "Epoch 323/500, Loss: 0.5455, Validation Loss: 0.9363\n",
      "Epoch 324/500, Loss: 0.5383, Validation Loss: 0.9312\n",
      "Epoch 325/500, Loss: 0.5324, Validation Loss: 0.9227\n",
      "Epoch 326/500, Loss: 0.5364, Validation Loss: 0.9344\n",
      "Epoch 327/500, Loss: 0.5337, Validation Loss: 0.9344\n",
      "Epoch 328/500, Loss: 0.5266, Validation Loss: 0.9326\n",
      "Epoch 329/500, Loss: 0.5283, Validation Loss: 0.9383\n",
      "Epoch 330/500, Loss: 0.5315, Validation Loss: 0.9303\n",
      "Epoch 331/500, Loss: 0.5334, Validation Loss: 0.9417\n",
      "Epoch 332/500, Loss: 0.5264, Validation Loss: 0.9423\n",
      "Epoch 333/500, Loss: 0.5246, Validation Loss: 0.9432\n",
      "Epoch 334/500, Loss: 0.5269, Validation Loss: 0.9453\n",
      "Epoch 335/500, Loss: 0.5253, Validation Loss: 0.9277\n",
      "Epoch 336/500, Loss: 0.5279, Validation Loss: 0.9395\n",
      "Epoch 337/500, Loss: 0.5191, Validation Loss: 0.9499\n",
      "Epoch 338/500, Loss: 0.5171, Validation Loss: 0.9457\n",
      "Epoch 339/500, Loss: 0.5184, Validation Loss: 0.9459\n",
      "Epoch 340/500, Loss: 0.5149, Validation Loss: 0.9455\n",
      "Epoch 341/500, Loss: 0.5158, Validation Loss: 0.9600\n",
      "Epoch 342/500, Loss: 0.5186, Validation Loss: 0.9463\n",
      "Epoch 343/500, Loss: 0.5348, Validation Loss: 0.9727\n",
      "Epoch 344/500, Loss: 0.5731, Validation Loss: 0.9455\n",
      "Epoch 345/500, Loss: 0.6166, Validation Loss: 0.9558\n",
      "Epoch 346/500, Loss: 0.5912, Validation Loss: 0.9289\n",
      "Epoch 347/500, Loss: 0.5312, Validation Loss: 0.9242\n",
      "Epoch 348/500, Loss: 0.5621, Validation Loss: 0.9624\n",
      "Epoch 349/500, Loss: 0.5793, Validation Loss: 0.9376\n",
      "Epoch 350/500, Loss: 0.5231, Validation Loss: 0.9342\n",
      "Epoch 351/500, Loss: 0.5588, Validation Loss: 0.9471\n",
      "Epoch 352/500, Loss: 0.5600, Validation Loss: 0.9308\n",
      "Epoch 353/500, Loss: 0.5319, Validation Loss: 0.9275\n",
      "Epoch 354/500, Loss: 0.5437, Validation Loss: 0.9475\n",
      "Epoch 355/500, Loss: 0.5859, Validation Loss: 0.9375\n",
      "Epoch 356/500, Loss: 0.5363, Validation Loss: 0.9389\n",
      "Epoch 357/500, Loss: 0.5637, Validation Loss: 0.9602\n",
      "Epoch 358/500, Loss: 0.5676, Validation Loss: 0.9241\n",
      "Epoch 359/500, Loss: 0.5350, Validation Loss: 0.9199\n",
      "Epoch 360/500, Loss: 0.5425, Validation Loss: 0.9455\n",
      "Epoch 361/500, Loss: 0.5747, Validation Loss: 0.9326\n",
      "Epoch 362/500, Loss: 0.5452, Validation Loss: 0.9227\n",
      "Epoch 363/500, Loss: 0.5809, Validation Loss: 0.9295\n",
      "Epoch 364/500, Loss: 0.6236, Validation Loss: 0.9137\n",
      "Epoch 365/500, Loss: 0.5691, Validation Loss: 0.9281\n",
      "Epoch 366/500, Loss: 0.5916, Validation Loss: 0.9684\n",
      "Epoch 367/500, Loss: 0.6120, Validation Loss: 0.9432\n",
      "Epoch 368/500, Loss: 0.5511, Validation Loss: 0.9294\n",
      "Epoch 369/500, Loss: 0.6196, Validation Loss: 0.9169\n",
      "Epoch 370/500, Loss: 0.5735, Validation Loss: 0.9320\n",
      "Epoch 371/500, Loss: 0.5724, Validation Loss: 0.9220\n",
      "Epoch 372/500, Loss: 0.5801, Validation Loss: 0.9247\n",
      "Epoch 373/500, Loss: 0.5407, Validation Loss: 0.9369\n",
      "Epoch 374/500, Loss: 0.5681, Validation Loss: 0.9383\n",
      "Epoch 375/500, Loss: 0.5576, Validation Loss: 0.9481\n",
      "Epoch 376/500, Loss: 0.5374, Validation Loss: 0.9566\n",
      "Epoch 377/500, Loss: 0.5510, Validation Loss: 0.9394\n",
      "Epoch 378/500, Loss: 0.5485, Validation Loss: 0.9535\n",
      "Epoch 379/500, Loss: 0.5357, Validation Loss: 0.9562\n",
      "Epoch 380/500, Loss: 0.5259, Validation Loss: 0.9526\n",
      "Epoch 381/500, Loss: 0.5315, Validation Loss: 0.9545\n",
      "Epoch 382/500, Loss: 0.5291, Validation Loss: 0.9549\n",
      "Epoch 383/500, Loss: 0.5227, Validation Loss: 0.9635\n",
      "Epoch 384/500, Loss: 0.5222, Validation Loss: 0.9643\n",
      "Epoch 385/500, Loss: 0.5194, Validation Loss: 0.9561\n",
      "Epoch 386/500, Loss: 0.5213, Validation Loss: 0.9692\n",
      "Epoch 387/500, Loss: 0.5427, Validation Loss: 0.9521\n",
      "Epoch 388/500, Loss: 0.5775, Validation Loss: 0.9722\n",
      "Epoch 389/500, Loss: 0.5379, Validation Loss: 0.9648\n",
      "Epoch 390/500, Loss: 0.5177, Validation Loss: 0.9559\n",
      "Epoch 391/500, Loss: 0.5404, Validation Loss: 0.9725\n",
      "Epoch 392/500, Loss: 0.5298, Validation Loss: 0.9603\n",
      "Epoch 393/500, Loss: 0.5111, Validation Loss: 0.9611\n",
      "Epoch 394/500, Loss: 0.5138, Validation Loss: 0.9771\n",
      "Epoch 395/500, Loss: 0.5214, Validation Loss: 0.9574\n",
      "Epoch 396/500, Loss: 0.5156, Validation Loss: 0.9596\n",
      "Epoch 397/500, Loss: 0.5046, Validation Loss: 0.9692\n",
      "Epoch 398/500, Loss: 0.5106, Validation Loss: 0.9603\n",
      "Epoch 399/500, Loss: 0.5186, Validation Loss: 0.9759\n",
      "Epoch 400/500, Loss: 0.5088, Validation Loss: 0.9677\n",
      "Epoch 401/500, Loss: 0.4984, Validation Loss: 0.9652\n",
      "Epoch 402/500, Loss: 0.5034, Validation Loss: 0.9758\n",
      "Epoch 403/500, Loss: 0.5067, Validation Loss: 0.9729\n",
      "Epoch 404/500, Loss: 0.5065, Validation Loss: 0.9675\n",
      "Epoch 405/500, Loss: 0.5108, Validation Loss: 0.9749\n",
      "Epoch 406/500, Loss: 0.5091, Validation Loss: 0.9719\n",
      "Epoch 407/500, Loss: 0.5080, Validation Loss: 0.9784\n",
      "Epoch 408/500, Loss: 0.5050, Validation Loss: 0.9746\n",
      "Epoch 409/500, Loss: 0.5035, Validation Loss: 0.9790\n",
      "Epoch 410/500, Loss: 0.5015, Validation Loss: 0.9836\n",
      "Epoch 411/500, Loss: 0.5018, Validation Loss: 0.9790\n",
      "Epoch 412/500, Loss: 0.5021, Validation Loss: 0.9840\n",
      "Epoch 413/500, Loss: 0.5125, Validation Loss: 0.9696\n",
      "Epoch 414/500, Loss: 0.5441, Validation Loss: 0.9967\n",
      "Epoch 415/500, Loss: 0.5685, Validation Loss: 0.9645\n",
      "Epoch 416/500, Loss: 0.5646, Validation Loss: 0.9640\n",
      "Epoch 417/500, Loss: 0.5147, Validation Loss: 0.9748\n",
      "Epoch 418/500, Loss: 0.5249, Validation Loss: 0.9637\n",
      "Epoch 419/500, Loss: 0.5522, Validation Loss: 0.9653\n",
      "Epoch 420/500, Loss: 0.5112, Validation Loss: 0.9634\n",
      "Epoch 421/500, Loss: 0.5214, Validation Loss: 0.9532\n",
      "Epoch 422/500, Loss: 0.5361, Validation Loss: 0.9616\n",
      "Epoch 423/500, Loss: 0.5066, Validation Loss: 0.9779\n",
      "Epoch 424/500, Loss: 0.5263, Validation Loss: 0.9736\n",
      "Epoch 425/500, Loss: 0.5419, Validation Loss: 0.9708\n",
      "Epoch 426/500, Loss: 0.5016, Validation Loss: 0.9761\n",
      "Epoch 427/500, Loss: 0.5286, Validation Loss: 0.9582\n",
      "Epoch 428/500, Loss: 0.5398, Validation Loss: 0.9689\n",
      "Epoch 429/500, Loss: 0.4984, Validation Loss: 0.9805\n",
      "Epoch 430/500, Loss: 0.5285, Validation Loss: 0.9743\n",
      "Epoch 431/500, Loss: 0.5151, Validation Loss: 0.9738\n",
      "Epoch 432/500, Loss: 0.4979, Validation Loss: 0.9766\n",
      "Epoch 433/500, Loss: 0.5152, Validation Loss: 0.9688\n",
      "Epoch 434/500, Loss: 0.5070, Validation Loss: 0.9745\n",
      "Epoch 435/500, Loss: 0.4953, Validation Loss: 0.9869\n",
      "Epoch 436/500, Loss: 0.5086, Validation Loss: 0.9797\n",
      "Epoch 437/500, Loss: 0.5124, Validation Loss: 0.9848\n",
      "Epoch 438/500, Loss: 0.4973, Validation Loss: 0.9813\n",
      "Epoch 439/500, Loss: 0.4981, Validation Loss: 0.9745\n",
      "Epoch 440/500, Loss: 0.5047, Validation Loss: 0.9899\n",
      "Epoch 441/500, Loss: 0.4935, Validation Loss: 0.9882\n",
      "Epoch 442/500, Loss: 0.4896, Validation Loss: 0.9788\n",
      "Epoch 443/500, Loss: 0.4957, Validation Loss: 0.9874\n",
      "Epoch 444/500, Loss: 0.4918, Validation Loss: 0.9827\n",
      "Epoch 445/500, Loss: 0.4863, Validation Loss: 0.9847\n",
      "Epoch 446/500, Loss: 0.4898, Validation Loss: 0.9955\n",
      "Epoch 447/500, Loss: 0.4998, Validation Loss: 0.9742\n",
      "Epoch 448/500, Loss: 0.5131, Validation Loss: 0.9880\n",
      "Epoch 449/500, Loss: 0.5159, Validation Loss: 0.9827\n",
      "Epoch 450/500, Loss: 0.5077, Validation Loss: 0.9834\n",
      "Epoch 451/500, Loss: 0.5049, Validation Loss: 0.9806\n",
      "Epoch 452/500, Loss: 0.5136, Validation Loss: 0.9645\n",
      "Epoch 453/500, Loss: 0.5488, Validation Loss: 0.9817\n",
      "Epoch 454/500, Loss: 0.5810, Validation Loss: 0.9647\n",
      "Epoch 455/500, Loss: 0.5866, Validation Loss: 0.9897\n",
      "Epoch 456/500, Loss: 0.5477, Validation Loss: 1.0146\n",
      "Epoch 457/500, Loss: 0.5802, Validation Loss: 0.9723\n",
      "Epoch 458/500, Loss: 0.6148, Validation Loss: 0.9591\n",
      "Epoch 459/500, Loss: 0.5515, Validation Loss: 0.9564\n",
      "Epoch 460/500, Loss: 0.5608, Validation Loss: 0.9564\n",
      "Epoch 461/500, Loss: 0.5723, Validation Loss: 0.9678\n",
      "Epoch 462/500, Loss: 0.5401, Validation Loss: 0.9752\n",
      "Epoch 463/500, Loss: 0.5522, Validation Loss: 0.9676\n",
      "Epoch 464/500, Loss: 0.5295, Validation Loss: 0.9725\n",
      "Epoch 465/500, Loss: 0.5324, Validation Loss: 0.9764\n",
      "Epoch 466/500, Loss: 0.5755, Validation Loss: 0.9526\n",
      "Epoch 467/500, Loss: 0.6196, Validation Loss: 0.9573\n",
      "Epoch 468/500, Loss: 0.5304, Validation Loss: 0.9668\n",
      "Epoch 469/500, Loss: 0.5740, Validation Loss: 0.9532\n",
      "Epoch 470/500, Loss: 0.5382, Validation Loss: 0.9613\n",
      "Epoch 471/500, Loss: 0.5398, Validation Loss: 0.9707\n",
      "Epoch 472/500, Loss: 0.5454, Validation Loss: 0.9695\n",
      "Epoch 473/500, Loss: 0.5199, Validation Loss: 0.9589\n",
      "Epoch 474/500, Loss: 0.5352, Validation Loss: 0.9587\n",
      "Epoch 475/500, Loss: 0.5098, Validation Loss: 0.9568\n",
      "Epoch 476/500, Loss: 0.5256, Validation Loss: 0.9597\n",
      "Epoch 477/500, Loss: 0.5047, Validation Loss: 0.9609\n",
      "Epoch 478/500, Loss: 0.5140, Validation Loss: 0.9718\n",
      "Epoch 479/500, Loss: 0.5011, Validation Loss: 0.9727\n",
      "Epoch 480/500, Loss: 0.4998, Validation Loss: 0.9665\n",
      "Epoch 481/500, Loss: 0.5142, Validation Loss: 0.9738\n",
      "Epoch 482/500, Loss: 0.5032, Validation Loss: 0.9764\n",
      "Epoch 483/500, Loss: 0.5062, Validation Loss: 0.9736\n",
      "Epoch 484/500, Loss: 0.5054, Validation Loss: 0.9815\n",
      "Epoch 485/500, Loss: 0.5039, Validation Loss: 0.9847\n",
      "Epoch 486/500, Loss: 0.5021, Validation Loss: 0.9833\n",
      "Epoch 487/500, Loss: 0.5039, Validation Loss: 0.9801\n",
      "Epoch 488/500, Loss: 0.5004, Validation Loss: 0.9810\n",
      "Epoch 489/500, Loss: 0.4975, Validation Loss: 0.9912\n",
      "Epoch 490/500, Loss: 0.4952, Validation Loss: 0.9978\n",
      "Epoch 491/500, Loss: 0.4981, Validation Loss: 1.0009\n",
      "Epoch 492/500, Loss: 0.4934, Validation Loss: 0.9934\n",
      "Epoch 493/500, Loss: 0.4968, Validation Loss: 0.9952\n",
      "Epoch 494/500, Loss: 0.4911, Validation Loss: 0.9953\n",
      "Epoch 495/500, Loss: 0.4902, Validation Loss: 0.9989\n",
      "Epoch 496/500, Loss: 0.4921, Validation Loss: 0.9997\n",
      "Epoch 497/500, Loss: 0.4956, Validation Loss: 1.0071\n",
      "Epoch 498/500, Loss: 0.5012, Validation Loss: 0.9909\n",
      "Epoch 499/500, Loss: 0.5291, Validation Loss: 0.9897\n",
      "Epoch 500/500, Loss: 0.5244, Validation Loss: 0.9820\n"
     ]
    }
   ],
   "source": [
    "# Define Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Model parameters\n",
    "input_dim = full_df.shape[1]\n",
    "encoding_dim = 3  #Reduced dimensionality\n",
    "model = Autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "epochs = 500\n",
    "batch_size = 10\n",
    "X_train_tensor = torch.tensor(train_df_scaled, dtype=torch.float32)\n",
    "\n",
    "def evaluate(model, test_df_scaled, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        X_test_tensor = torch.tensor(test_df_scaled, dtype=torch.float32)\n",
    "        outputs = model(X_test_tensor)  # Get decoded output\n",
    "        val_loss = criterion(outputs[1], X_test_tensor)  # Compare reconstructed data\n",
    "    return val_loss.item()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    reconstructed = model(X_train_tensor)[1]\n",
    "    loss = criterion(reconstructed, X_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    val_loss = evaluate(model, test_df_scaled, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the behavioral data to lower dimensions\n",
    "with torch.no_grad():\n",
    "    X_train_reduced = model.encoder(X_train_tensor).numpy()\n",
    "    X_test_tensor = torch.tensor(test_df_scaled, dtype=torch.float32)\n",
    "    X_test_reduced = model.encoder(X_test_tensor).numpy()\n",
    "\n",
    "test_df_reduced = pd.DataFrame(\n",
    "        X_test_reduced,\n",
    "        index=test_df.index,\n",
    "        columns=[\"component_1\", \"component_2\", \"component_3\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 15:18:10,845 [ WARNING] /home/haotsung/HCP_behavioral_prediction/julearn_hcp/lib/python3.11/site-packages/julearn/prepare.py:195: RuntimeWarning: The dataframe has 80203 columns. Checking X for consistency might take a while. To skip this checks, set the config flag `disable_x_check` to `True`.\n",
      "  warn_with_log(\n",
      "\n",
      "2025-01-16 15:37:54,991 [ WARNING] /home/haotsung/HCP_behavioral_prediction/julearn_hcp/lib/python3.11/site-packages/julearn/prepare.py:471: RuntimeWarning: X has 79800 columns. Checking X_types for consistency might take a while. To skip this checks, set the config flag `disable_xtypes_check` to `True`.\n",
      "  warn_with_log(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_corr:\n",
      " 0     8.941516e-04\n",
      "1     3.442660e-03\n",
      "2     1.162774e-02\n",
      "3     3.633204e-04\n",
      "4     3.177245e-03\n",
      "5     1.165064e-02\n",
      "6     1.066333e-04\n",
      "7     1.946646e-03\n",
      "8     7.974767e-03\n",
      "9     8.957556e-03\n",
      "10    2.904341e-03\n",
      "11    3.149623e-03\n",
      "12    2.982044e-05\n",
      "13    1.769274e-03\n",
      "14    1.012621e-02\n",
      "15    4.472219e-09\n",
      "16    5.508813e-04\n",
      "17    1.977997e-02\n",
      "18    1.910536e-02\n",
      "19    2.318740e-02\n",
      "20    2.005093e-02\n",
      "21    1.342179e-04\n",
      "22    1.102300e-02\n",
      "23    6.934613e-03\n",
      "24    2.174031e-02\n",
      "25    1.848244e-02\n",
      "26    4.775650e-03\n",
      "27    2.921824e-04\n",
      "28    4.770307e-03\n",
      "29    5.983361e-04\n",
      "30    9.828886e-03\n",
      "31    6.030415e-02\n",
      "32    8.826164e-04\n",
      "33    1.536766e-03\n",
      "34    1.223135e-02\n",
      "35    9.692623e-04\n",
      "36    6.620950e-03\n",
      "37    8.303500e-05\n",
      "38    2.040788e-03\n",
      "39    5.181090e-04\n",
      "40    8.778065e-03\n",
      "41    1.272808e-02\n",
      "42    8.119800e-03\n",
      "43    4.959147e-04\n",
      "44    8.442090e-03\n",
      "45    7.363955e-03\n",
      "46    2.469838e-04\n",
      "47    1.240929e-03\n",
      "48    1.623200e-02\n",
      "49    1.305725e-04\n",
      "Name: test_r2_corr, dtype: float64\n",
      "r_corr:\n",
      " 0     0.029902\n",
      "1     0.058674\n",
      "2    -0.107832\n",
      "3     0.019061\n",
      "4    -0.056367\n",
      "5     0.107938\n",
      "6     0.010326\n",
      "7    -0.044121\n",
      "8     0.089302\n",
      "9    -0.094644\n",
      "10   -0.053892\n",
      "11   -0.056121\n",
      "12   -0.005461\n",
      "13    0.042063\n",
      "14    0.100629\n",
      "15    0.000067\n",
      "16    0.023471\n",
      "17   -0.140641\n",
      "18    0.138222\n",
      "19    0.152274\n",
      "20    0.141601\n",
      "21    0.011585\n",
      "22    0.104990\n",
      "23   -0.083274\n",
      "24   -0.147446\n",
      "25   -0.135950\n",
      "26    0.069106\n",
      "27   -0.017093\n",
      "28    0.069067\n",
      "29   -0.024461\n",
      "30    0.099141\n",
      "31   -0.245569\n",
      "32   -0.029709\n",
      "33   -0.039202\n",
      "34   -0.110595\n",
      "35    0.031133\n",
      "36    0.081369\n",
      "37    0.009112\n",
      "38    0.045175\n",
      "39   -0.022762\n",
      "40    0.093691\n",
      "41   -0.112819\n",
      "42    0.090110\n",
      "43    0.022269\n",
      "44   -0.091881\n",
      "45   -0.085813\n",
      "46   -0.015716\n",
      "47    0.035227\n",
      "48    0.127405\n",
      "49    0.011427\n",
      "Name: test_r_corr, dtype: float64\n",
      "Scores with best hyperparameter: 0.0018593777741319237\n"
     ]
    }
   ],
   "source": [
    "# %% Feature generation\n",
    "storage = HDF5FeatureStorage(\"features.hdf5\")\n",
    "df_features = storage.read_df('BOLD_Schaefer400x17_functional_connectivity')\n",
    "\n",
    "# Group by 'subject' and calculate the mean across REST1/LR, REST1/RL, REST2/LR, REST2/RL \n",
    "df_features = df_features.groupby('subject').mean()\n",
    "\n",
    "# merge the dataframe\n",
    "df_features.index.name= \"Subject\"\n",
    "df_data = df_features.join(test_df_reduced, how=\"inner\")\n",
    "\n",
    "# %%\n",
    "# Regression model training\n",
    "# Step 1: Seperate features (X) and targets (y)\n",
    "X = [x for x in df_data.columns if \"~\" in x]\n",
    "X = [x for x in X if x.split(\"~\")[0] != x.split(\"~\")[1]]\n",
    "y = \"component_2\"\n",
    "X_types = {\n",
    "    \"features\":[\".*~.*\"],\n",
    "} \n",
    "# %%\n",
    "# Create the pipeline that will be used to predict the target.\n",
    "kernel_ridge_model = KernelRidge()\n",
    "creator = PipelineCreator(problem_type=\"regression\",apply_to=\"features\")\n",
    "creator.add(\"zscore\")\n",
    "creator.add(\n",
    "    kernel_ridge_model, \n",
    "    alpha=10, \n",
    "    kernel='linear', \n",
    "    )\n",
    "# %%\n",
    "# Evaluate the model within the cross validation.\n",
    "rkf = RepeatedKFold(n_splits=10,n_repeats=5,random_state=42)\n",
    "scores_tuned, model_tuned = run_cross_validation(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    X_types=X_types,\n",
    "    data=df_data,\n",
    "    model=creator, \n",
    "    return_estimator=\"all\",\n",
    "    cv=rkf,\n",
    "    scoring= ['r2_corr','r_corr'],\n",
    "    n_jobs = -1,\n",
    ")\n",
    "print(\"r2_corr:\\n\",scores_tuned[\"test_r2_corr\"])\n",
    "print(\"r_corr:\\n\",scores_tuned[\"test_r_corr\"])\n",
    "print(f\"Scores with best hyperparameter: {scores_tuned['test_r_corr'].mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julearn_hcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
